{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.5/dist-packages (1.16.2)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.5/dist-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.16.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.5/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.5/dist-packages (from sklearn) (0.21.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.5/dist-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.5/dist-packages (2.2.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras) (5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras) (1.2.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras) (2.9.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.5/dist-packages (3.0.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (1.16.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from kiwisolver>=1.0.1->matplotlib) (40.8.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install sklearn\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from utils import TrainingPlot, TimeSummary, plot_training_summary\n",
    "from NNKeras import NNKeras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import inspect\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Step 1\n",
    "### Data Preparation\n",
    "\n",
    "#### Data Cleanup\n",
    "-  Merge 64 size vectors by ignoring line breaks\n",
    "-  Create subset of data by selecting non consecutive vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Cleaned data\n",
    "L42023,0.04347826,0.04347826,0.,0.04347826,0.01086957,0.02173913,0.,0.02173913,0.,0.,0.,0.,0.,0.02173913,0.02173913,0.04347826,0.07608696,0.02173913,0.,0.0326087,0.01086957,0.,0.,0.0326087,0.,0.01086957,0.,0.0326087,0.,0.,0.,0.0326087,0.05434783,0.,0.01086957,0.02173913,0.04347826,0.,0.01086957,0.02173913,0.02173913,0.,0.,0.01086957,0.0326087,0.,0.04347826,0.0326087,0.01086957,0.01086957,0.,0.02173913,0.04347826,0.01086957,0.,0.01086957,0.,0.,0.,0.,0.04347826,0.02173913,0.,0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "## Network 1\n",
    "### Create input dataset X and y where X has all vectors and y is one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "nn = NNKeras(\"/tf/dataset/dataset1.csv\")\n",
    "X, y, unique_classes = nn.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Vector Shape\n",
      "(81068, 64)\n",
      "Initialized Training Vector Shape\n",
      "(81068, 31)\n",
      "Input Vector Head\n",
      "         1         2         3         4         5         6         7   \\\n",
      "0  0.045455  0.045455  0.000000  0.000000  0.045455  0.318182  0.000000   \n",
      "1  0.055556  0.015873  0.047619  0.000000  0.000000  0.000000  0.023810   \n",
      "2  0.027316  0.021378  0.015439  0.027316  0.004751  0.024941  0.008314   \n",
      "\n",
      "         8         9         10  ...        55        56        57        58  \\\n",
      "0  0.000000  0.000000  0.045455  ...  0.000000  0.000000  0.045455  0.000000   \n",
      "1  0.000000  0.031746  0.023810  ...  0.007937  0.000000  0.000000  0.015873   \n",
      "2  0.005938  0.000000  0.013064  ...  0.011876  0.010689  0.001188  0.010689   \n",
      "\n",
      "         59        60        61       62        63        64  \n",
      "0  0.000000  0.000000  0.000000  0.00000  0.000000  0.000000  \n",
      "1  0.015873  0.023810  0.007937  0.02381  0.015873  0.023810  \n",
      "2  0.004751  0.004751  0.013064  0.02019  0.016627  0.016627  \n",
      "\n",
      "[3 rows x 64 columns]\n",
      "Training Vector Head\n",
      "[[ True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False]\n",
      " [ True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False]\n",
      " [ True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False]]\n",
      "Categories\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AE014075</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE006470</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AE002161</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AE000512</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AE009442</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AE002098</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AE000666</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BA000002</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AL450380</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AE000516</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>BA000004</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AE001437</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AL139299</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>L77117</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AB001339</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AE015924</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AE000520</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>AE005672</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NC_00918</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>CP000025</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>AL645882</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>L42023</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AE004092</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>CP000241</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NC_000961</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AE003852</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AE004091</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>AE003853</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>AE000782</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AL096836</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>AL009126</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0  indices\n",
       "0    AE014075        1\n",
       "1    AE006470        2\n",
       "2    AE002161        3\n",
       "3    AE000512        4\n",
       "4    AE009442        5\n",
       "5    AE002098        6\n",
       "6    AE000666        7\n",
       "7    BA000002        8\n",
       "8    AL450380        9\n",
       "9    AE000516       10\n",
       "10   BA000004       11\n",
       "11   AE001437       12\n",
       "12   AL139299       13\n",
       "13     L77117       14\n",
       "14   AB001339       15\n",
       "15   AE015924       16\n",
       "16   AE000520       17\n",
       "17   AE005672       18\n",
       "18   NC_00918       19\n",
       "19   CP000025       20\n",
       "20   AL645882       21\n",
       "21     L42023       22\n",
       "22   AE004092       23\n",
       "23   CP000241       24\n",
       "24  NC_000961       25\n",
       "25   AE003852       26\n",
       "26   AE004091       27\n",
       "27   AE003853       28\n",
       "28   AE000782       29\n",
       "29   AL096836       30\n",
       "30   AL009126       31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Input Vector Shape\")\n",
    "print(X.shape)\n",
    "print(\"Initialized Training Vector Shape\")\n",
    "print(y.shape)\n",
    "print(\"Input Vector Head\")\n",
    "print(X[:3])\n",
    "print(\"Training Vector Head\")\n",
    "print(y[:3])\n",
    "print(\"Categories\")\n",
    "unique_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Prepare Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             1         2        3         4         5    6     7         8   \\\n",
      "61310  0.078431  0.024510  0.02451  0.024510  0.000000  0.0  0.00  0.004902   \n",
      "28317  0.040000  0.000000  0.02000  0.020000  0.000000  0.0  0.04  0.000000   \n",
      "48590  0.032787  0.016393  0.00000  0.065574  0.016393  0.0  0.00  0.049180   \n",
      "\n",
      "         9         10  ...    55        56   57   58   59   60        61  \\\n",
      "61310  0.00  0.024510  ...  0.00  0.029412  0.0  0.0  0.0  0.0  0.029412   \n",
      "28317  0.04  0.040000  ...  0.04  0.020000  0.0  0.0  0.0  0.0  0.060000   \n",
      "48590  0.00  0.016393  ...  0.00  0.081967  0.0  0.0  0.0  0.0  0.016393   \n",
      "\n",
      "             62        63        64  \n",
      "61310  0.019608  0.073529  0.068627  \n",
      "28317  0.020000  0.020000  0.040000  \n",
      "48590  0.000000  0.049180  0.000000  \n",
      "\n",
      "[3 rows x 64 columns]\n",
      "[[False False False False False False False False False False False False\n",
      "  False False False False False False False False False False False  True\n",
      "  False False False False False False False]\n",
      " [False False False False False False False False False False  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False  True False False False False\n",
      "  False False False False False False False]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "print(X_train[:3])\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accurracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9b009cd778eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0maccurracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accurracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = SVR(kernel=\"linear\")\n",
    "model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8063401998273098"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred, np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc 0.8063401998273098\n",
      "train acc 0.8080149258334104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "y_pred = model.predict(X_test)\n",
    "print('test acc', accuracy_score(y_pred, np.argmax(y_test, axis=1)))\n",
    "y_pred_train = model.predict(X_train)\n",
    "print('train acc', accuracy_score(y_pred_train, np.argmax(y_train, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6276840363088044,\n",
       " 0.12842335790758083,\n",
       " 0.9855493132068727,\n",
       " 0.9747015654057375,\n",
       " 0.9846858223152614,\n",
       " 0.22793453353646811,\n",
       " 0.9525304439893207,\n",
       " 0.9848064758193165,\n",
       " 0.06279694838862672,\n",
       " 0.9942703040237872,\n",
       " 0.997244874050811,\n",
       " 0.9999430012032656,\n",
       " 0.995506267427821,\n",
       " 0.001237050290298243,\n",
       " 0.9999691030481185,\n",
       " 0.9760547429386491,\n",
       " 0.9935584429334371,\n",
       " 0.9968865496493972,\n",
       " 0.9999902662680498,\n",
       " 0.9974939226837329,\n",
       " 0.09263398136651102,\n",
       " 0.5049866628818243,\n",
       " 0.6760756698310739,\n",
       " 0.9480856683197805,\n",
       " 0.8594753520785129,\n",
       " 0.0012480291017221183,\n",
       " 0.21457095395765458,\n",
       " 0.6419479186457224,\n",
       " 0.9862923667064881,\n",
       " 0.9970542877695857,\n",
       " 0.9989848065764967,\n",
       " 0.9995094563471323,\n",
       " 0.9981742150834064,\n",
       " 0.8687302930495695,\n",
       " 0.9908297467847694,\n",
       " 1.423078079593366e-08,\n",
       " 0.00017549163889312013,\n",
       " 0.9964147169798525,\n",
       " 0.9980088783579583,\n",
       " 0.05779092419082558,\n",
       " 0.847015427660426,\n",
       " 0.6916671004832828,\n",
       " 0.9960114650548987,\n",
       " 0.9746158023664603,\n",
       " 0.8817653252994746,\n",
       " 0.9988808744382445,\n",
       " 0.9837566415541376,\n",
       " 0.9911535559844468,\n",
       " 0.9672761438915409,\n",
       " 0.02318913065940802,\n",
       " 0.7895062041751791,\n",
       " 0.9979095261062819,\n",
       " 0.9989247947537222,\n",
       " 0.81300935242488,\n",
       " 0.0014874490805683173,\n",
       " 0.0068930829780985605,\n",
       " 0.999466077913343,\n",
       " 0.22044633085637547,\n",
       " 0.8549109955726951,\n",
       " 0.9970970642120061,\n",
       " 0.9821853972837563,\n",
       " 0.9999221981175981,\n",
       " 0.9901400444641508,\n",
       " 0.9996063836865101,\n",
       " 0.9997123015298273,\n",
       " 0.9999700562857078,\n",
       " 0.9976385148038744,\n",
       " 0.20658538663928866,\n",
       " 0.9931320897083166,\n",
       " 0.9997160159550763,\n",
       " 0.9974516107991332,\n",
       " 0.98113268701513,\n",
       " 0.028321249129811123,\n",
       " 5.715152622128999e-06,\n",
       " 1.5300708162688753e-06,\n",
       " 0.021946576022319135,\n",
       " 0.8990667977884116,\n",
       " 0.8408533019774345,\n",
       " 0.9954420802572178,\n",
       " 0.9903944413149757,\n",
       " 0.9960128222583077,\n",
       " 0.07823245084620213,\n",
       " 0.9981238541137585,\n",
       " 0.23473434776964552,\n",
       " 0.991807712711397,\n",
       " 0.9943297011046409,\n",
       " 0.9762939882606202,\n",
       " 7.2373200056848e-11,\n",
       " 0.00011939087833314095,\n",
       " 0.25072031121399857,\n",
       " 9.203603859150764e-07,\n",
       " 0.9762474317271193,\n",
       " 0.2839091223331053,\n",
       " 0.7920908416945037,\n",
       " 0.21969790411105472,\n",
       " 0.9776809192120988,\n",
       " 0.9689020470698001,\n",
       " 0.8636601791512717,\n",
       " 0.509779654709629,\n",
       " 0.9930175937380209,\n",
       " 0.9978580662044305,\n",
       " 0.9996673523523644,\n",
       " 0.9999191393187303,\n",
       " 0.9996905011119728,\n",
       " 0.9947936118759257,\n",
       " 0.9607162716594643,\n",
       " 0.9996449852034789,\n",
       " 0.9931238686222429,\n",
       " 0.9832799284218523,\n",
       " 0.9658955346362441,\n",
       " 0.9911900938044066,\n",
       " 0.9975236016490502,\n",
       " 0.6205244008173696,\n",
       " 0.9998053793255836,\n",
       " 0.9919761932564477,\n",
       " 0.10452439257551846,\n",
       " 0.0005144473410169019,\n",
       " 0.008496475848658713,\n",
       " 0.9838137317295803,\n",
       " 0.9986719932443989,\n",
       " 0.9393176548705567,\n",
       " 0.9817096741021539,\n",
       " 0.9844571001453855,\n",
       " 0.8921387285833731,\n",
       " 0.056563208624908635,\n",
       " 0.865409647783172,\n",
       " 0.9947779325322265,\n",
       " 0.976663161956253,\n",
       " 0.994373144240823,\n",
       " 0.00027652695187029637,\n",
       " 3.853787868768412e-11,\n",
       " 6.341998726026914e-08,\n",
       " 4.428966365907535e-07,\n",
       " 0.9769575220205974,\n",
       " 0.9192418317995377,\n",
       " 0.5689442938394924,\n",
       " 0.035025210875456694,\n",
       " 0.9999251743864853,\n",
       " 0.9972600203654618,\n",
       " 0.8241732705350078,\n",
       " 0.999942407158042,\n",
       " 0.8340681708988962,\n",
       " 0.9968710300954706,\n",
       " 0.9995194795941379,\n",
       " 0.8102524843842558,\n",
       " 0.9970754300319216,\n",
       " 0.9997157105448184,\n",
       " 0.9982418202541566,\n",
       " 0.9403914128870121,\n",
       " 0.9890128309496451,\n",
       " 0.8350318093962004,\n",
       " 0.48443953909027293,\n",
       " 0.8740394919125665,\n",
       " 0.9035115106990548,\n",
       " 0.639865864409811,\n",
       " 0.3532967393987172,\n",
       " 0.024042068901953755,\n",
       " 0.9951799212021464,\n",
       " 0.9940485884511682,\n",
       " 0.013930987044646962,\n",
       " 0.9771866896308179,\n",
       " 0.06084766619257016,\n",
       " 0.0011470471709105125,\n",
       " 0.5140834415421506,\n",
       " 0.7435919299216217,\n",
       " 0.9463112520449506,\n",
       " 9.76342145720863e-06,\n",
       " 2.8891432569492993e-06,\n",
       " 0.9707593558639203,\n",
       " 0.9847902098742417,\n",
       " 0.9386184700157832,\n",
       " 0.9990551238467009,\n",
       " 0.31141763909842823,\n",
       " 0.18329781652900826,\n",
       " 0.8663937670732491,\n",
       " 0.0008196589891012716,\n",
       " 0.9976319589444906,\n",
       " 0.3929790907437574,\n",
       " 0.9991017344811761,\n",
       " 0.9995478662816774,\n",
       " 0.9756486528503213,\n",
       " 0.9956989697481572,\n",
       " 0.9965846712159827,\n",
       " 0.9690395235780895,\n",
       " 0.9987807593859509,\n",
       " 0.7885981179222357,\n",
       " 0.9193915210030043,\n",
       " 0.9108343350872179,\n",
       " 0.9957174511442832,\n",
       " 0.9954695243399103,\n",
       " 0.9999773363820869,\n",
       " 0.07823245084620213,\n",
       " 0.9757777750762868,\n",
       " 0.9993421469864052,\n",
       " 0.9997742705483035,\n",
       " 0.9800702627768079,\n",
       " 0.7966510656377944,\n",
       " 0.9984074058335474,\n",
       " 0.9916577629057809,\n",
       " 0.9939564082496083,\n",
       " 0.9988123028747524,\n",
       " 0.9999315115076951,\n",
       " 0.2504550839595957,\n",
       " 0.8358811523327023,\n",
       " 0.8439713625908437,\n",
       " 0.9991064921116768,\n",
       " 0.9999990820793535,\n",
       " 0.9694533808074643,\n",
       " 0.9989401422911083,\n",
       " 0.9966383897545023,\n",
       " 0.9727408474203619,\n",
       " 0.9943529751258244,\n",
       " 0.9946563399886046,\n",
       " 0.8064822283445274,\n",
       " 0.9991845622274778,\n",
       " 0.9723701473079458,\n",
       " 0.951694253490772,\n",
       " 0.9929111724841421,\n",
       " 0.9716942422785738,\n",
       " 0.1521752251467117,\n",
       " 0.9985812714709172,\n",
       " 0.990352292206058,\n",
       " 0.9961311955736903,\n",
       " 0.0012842103296952542,\n",
       " 0.12280464680584831,\n",
       " 0.6194497649025036,\n",
       " 0.9998943392172738,\n",
       " 0.9988898272957789,\n",
       " 0.9953466836747407,\n",
       " 0.9983118974871013,\n",
       " 0.999894806057735,\n",
       " 0.9986199005039882,\n",
       " 0.9962373679447561,\n",
       " 0.9171949737919872,\n",
       " 0.11929865843480592,\n",
       " 0.9594331484207059,\n",
       " 0.9351458630897169,\n",
       " 0.9884281219778145,\n",
       " 0.00033908237907436775,\n",
       " 0.7298781936906833,\n",
       " 0.9213400385849679,\n",
       " 0.7823949270160515,\n",
       " 2.853240364737471e-05,\n",
       " 0.008861213621752745,\n",
       " 0.919476451167771,\n",
       " 0.3786476857751853,\n",
       " 0.9459502707098979,\n",
       " 0.9997920988663723,\n",
       " 0.9902458000435027,\n",
       " 0.03667067220617747,\n",
       " 0.2754471867097695,\n",
       " 0.9996557679772597,\n",
       " 0.9950698473756594,\n",
       " 0.980687928987175,\n",
       " 0.9999141045250303,\n",
       " 0.9126101254156879,\n",
       " 0.8952380322393206,\n",
       " 0.998276624523296,\n",
       " 0.971066135175982,\n",
       " 0.9858749981078664,\n",
       " 0.9995896438929629,\n",
       " 0.9957939186449214,\n",
       " 0.9973481965803308,\n",
       " 0.997241024346308,\n",
       " 0.9952554232154768,\n",
       " 0.9994811113678017,\n",
       " 0.11962382498180578,\n",
       " 0.8203896117597221,\n",
       " 0.47423057440436767,\n",
       " 0.07067279182789708,\n",
       " 0.9917419939691273,\n",
       " 0.9981533501817818,\n",
       " 0.9957525593155779,\n",
       " 0.9966862907639374,\n",
       " 0.9830198328334245,\n",
       " 5.960801511038229e-07,\n",
       " 0.11842319726955249,\n",
       " 0.4144946049852304,\n",
       " 0.00813455148728374,\n",
       " 1.4168437453932674e-05,\n",
       " 0.1919928864507629,\n",
       " 0.9981229104680432,\n",
       " 0.9996307268218461,\n",
       " 0.9981466592751311,\n",
       " 0.9911763464671023,\n",
       " 0.07398343207738221,\n",
       " 0.15351250171131445,\n",
       " 0.004081587353178369,\n",
       " 0.9519078582704139,\n",
       " 0.4888440651085696,\n",
       " 0.6903966371892126,\n",
       " 0.11752187152963882,\n",
       " 0.9999950295036618,\n",
       " 0.9999697363671957,\n",
       " 0.22353152793922423,\n",
       " 0.781892516540946,\n",
       " 0.9767553448623898,\n",
       " 0.004213462449343966,\n",
       " 0.7698973070678174,\n",
       " 0.9876194419435348,\n",
       " 0.08158547202620486,\n",
       " 0.015761089118952094,\n",
       " 0.003687523993426982,\n",
       " 0.7670930316580248,\n",
       " 0.3705092769310005,\n",
       " 0.6276788169374328,\n",
       " 0.24632032327905493,\n",
       " 0.8006394427281522,\n",
       " 0.16538935691295759,\n",
       " 0.03958937741384436,\n",
       " 0.03872074795072822,\n",
       " 0.928589324407052,\n",
       " 0.0022017293982960003,\n",
       " 2.1934121827384048e-08,\n",
       " 0.011176584846439191,\n",
       " 0.013327378757628551,\n",
       " 0.00037550358792184295,\n",
       " 0.5351154965582615,\n",
       " 0.4655103446201286,\n",
       " 0.904467713750493,\n",
       " 0.738825325908736,\n",
       " 0.9905743695813621,\n",
       " 0.8766890254565463,\n",
       " 0.9632826938950165,\n",
       " 0.975308949828244,\n",
       " 2.2078108882544437e-05,\n",
       " 0.03291350523280969,\n",
       " 0.9635650673000309,\n",
       " 0.8315864354317707,\n",
       " 0.9996804606942716,\n",
       " 0.777547066729675,\n",
       " 0.9657209465394817,\n",
       " 0.9899574812950234,\n",
       " 0.00017015763773354606,\n",
       " 0.21158585582241057,\n",
       " 0.9769575220205974,\n",
       " 0.9743913050371167,\n",
       " 0.9291203290383674,\n",
       " 0.09061902747608318,\n",
       " 0.02006796629873679,\n",
       " 0.1816140634462281,\n",
       " 0.9934389631643314,\n",
       " 0.9748408227234227,\n",
       " 0.9915531111993554,\n",
       " 9.9596178485472e-06,\n",
       " 0.00030426551407257326,\n",
       " 0.0025123424053027916,\n",
       " 0.43128059914305783,\n",
       " 6.019690582136213e-06,\n",
       " 0.7506714675898428,\n",
       " 0.929924683848029,\n",
       " 0.9993170300126476,\n",
       " 0.98334062819687,\n",
       " 0.9956064930164809,\n",
       " 0.917101612203033,\n",
       " 0.9964265087839331,\n",
       " 0.8194128520629651,\n",
       " 0.9965828769120529,\n",
       " 0.9981782036747574,\n",
       " 0.985120130516678,\n",
       " 0.49861618783885897,\n",
       " 0.9206228086908758,\n",
       " 0.9997477391710614,\n",
       " 0.9999227667140175,\n",
       " 0.9995103509363076,\n",
       " 0.09372966473526838,\n",
       " 0.969925670057961,\n",
       " 0.9998884212740535,\n",
       " 0.9414746338029284,\n",
       " 0.9980824437666236,\n",
       " 0.9963617026204277,\n",
       " 0.001303338036520112,\n",
       " 1.5363837677125465e-05,\n",
       " 0.9919141087657412,\n",
       " 0.9913260752928479,\n",
       " 1.1501688086131874e-06,\n",
       " 0.9374384366210293,\n",
       " 0.010472314618474648,\n",
       " 0.018192743190203607,\n",
       " 0.19333296495353747,\n",
       " 0.9349273676755199,\n",
       " 0.9526339821609432,\n",
       " 0.9859486711971728,\n",
       " 0.38638922306686746,\n",
       " 0.9898571537448199,\n",
       " 0.043672846941375125,\n",
       " 0.9985131882693621,\n",
       " 0.026432881792779726,\n",
       " 4.2248690669958264e-05,\n",
       " 0.24350922604517328,\n",
       " 0.07015254165002,\n",
       " 0.1852066605197574,\n",
       " 0.11402407069634332,\n",
       " 0.9954121653854721,\n",
       " 0.9907498267215621,\n",
       " 0.9860491123209357,\n",
       " 0.02564329175516585,\n",
       " 0.9357883421524359,\n",
       " 0.9190277097221189,\n",
       " 4.288247292965504e-05,\n",
       " 0.7235005055550688,\n",
       " 0.5090881021102756,\n",
       " 0.997061146629593,\n",
       " 0.9571021252603552,\n",
       " 0.9779101702182987,\n",
       " 0.24082584734184162,\n",
       " 0.9736997992222705,\n",
       " 0.27376627686271127,\n",
       " 0.14998313451309728,\n",
       " 0.9992822596266265,\n",
       " 0.023415276443761485,\n",
       " 0.9893573267414965,\n",
       " 0.9913458287513107,\n",
       " 0.9987722561537095,\n",
       " 0.9767802638593368,\n",
       " 0.9953551063191284,\n",
       " 0.9982308809687491,\n",
       " 0.8286256794852463,\n",
       " 6.732763728773531e-06,\n",
       " 0.9987317349039364,\n",
       " 0.9959672877134323,\n",
       " 0.9221447042798657,\n",
       " 0.9929805729872067,\n",
       " 0.9933256715039404,\n",
       " 0.006934574016950593,\n",
       " 0.9730872169269538,\n",
       " 0.03278620034659471,\n",
       " 0.037660583209136574,\n",
       " 0.9994609615212223,\n",
       " 0.9970545585508145,\n",
       " 0.9918546644326565,\n",
       " 0.9877641726557252,\n",
       " 0.9865984669328698,\n",
       " 0.9691346654288636,\n",
       " 0.9904680277310971,\n",
       " 0.014397829901293804,\n",
       " 0.9987480301345714,\n",
       " 0.9990255862560862,\n",
       " 0.13824273130951367,\n",
       " 0.2511653023354399,\n",
       " 0.31525034016478215,\n",
       " 0.6106206118816679,\n",
       " 0.9989324249277985,\n",
       " 0.7850712579929298,\n",
       " 0.9370962246943748,\n",
       " 0.9151646659491073,\n",
       " 0.8814117077699396,\n",
       " 0.5548656612952303,\n",
       " 0.0009718439883893141,\n",
       " 0.9965606834464577,\n",
       " 0.9968515524679304,\n",
       " 5.279251814087669e-07,\n",
       " 0.9998310505419228,\n",
       " 0.9896114196209651,\n",
       " 0.9784534846766452,\n",
       " 0.9977275652853956,\n",
       " 0.005380372219319167,\n",
       " 0.9963279944732856,\n",
       " 0.5287979071371666,\n",
       " 0.9897685591342213,\n",
       " 0.9897902756383445,\n",
       " 0.98727941634434,\n",
       " 0.011830712772795121,\n",
       " 0.3467445867248535,\n",
       " 0.9908712206398287,\n",
       " 1.8494242169229473e-05,\n",
       " 0.10631876588898137,\n",
       " 0.9795763079435323,\n",
       " 0.6722048542455135,\n",
       " 0.9851621787289673,\n",
       " 0.7106408860069179,\n",
       " 0.5880446190423351,\n",
       " 0.7661873004206117,\n",
       " 0.20576743811599177,\n",
       " 0.0064885661463600225,\n",
       " 0.9499572258302276,\n",
       " 0.19579444473739127,\n",
       " 0.9430461072133642,\n",
       " 0.0007108494566248338,\n",
       " 0.9971883029582219,\n",
       " 0.9166190840796828,\n",
       " 0.28837861642002405,\n",
       " 0.9652200018855956,\n",
       " 0.9967802912175757,\n",
       " 0.9052346840014682,\n",
       " 0.9521916273720101,\n",
       " 0.9978452399497026,\n",
       " 0.0007239998727317088,\n",
       " 0.9999155617664217,\n",
       " 0.9822278783533336,\n",
       " 0.9965800884143434,\n",
       " 0.9202025725725274,\n",
       " 0.972064903268402,\n",
       " 0.978777657218119,\n",
       " 0.9983313487550998,\n",
       " 0.9997844541669922,\n",
       " 0.9764023779330252,\n",
       " 0.961865792859893,\n",
       " 0.9989993642349084,\n",
       " 0.994775132413005,\n",
       " 0.9789988270749468,\n",
       " 0.7649639369902405,\n",
       " 0.9995110111562766,\n",
       " 0.6729695212946661,\n",
       " 0.9992368626361532,\n",
       " 0.9827028743373013,\n",
       " 0.9985923376604441,\n",
       " 0.9930588071837156,\n",
       " 0.9971652931719053,\n",
       " 0.9890280728697197,\n",
       " 0.6198475733450128,\n",
       " 0.9979526408365312,\n",
       " 0.9979777970371457,\n",
       " 0.9788354333412037,\n",
       " 0.9941144382009651,\n",
       " 0.9499504833721605,\n",
       " 0.8504715398405126,\n",
       " 0.9687830119575819,\n",
       " 0.7431321714583414,\n",
       " 0.9979229036400711,\n",
       " 0.9134854626224369,\n",
       " 0.9888118954063342,\n",
       " 0.9996238580369292,\n",
       " 0.9997798161513216,\n",
       " 0.8087413806759509,\n",
       " 0.7589344479224323,\n",
       " 0.09473963279365652,\n",
       " 0.8615187363315634,\n",
       " 0.9915003905529544,\n",
       " 0.7212310316227031,\n",
       " 0.9991330224460855,\n",
       " 0.9997484171034803,\n",
       " 0.9997984701681969,\n",
       " 0.7860220309711562,\n",
       " 0.9934607451429911,\n",
       " 0.9988638066379297,\n",
       " 0.9557204919649767,\n",
       " 0.9989506809468294,\n",
       " 0.9946644027181263,\n",
       " 0.7678594030433734,\n",
       " 0.20658538663928866,\n",
       " 0.9938027789010143,\n",
       " 0.9738135605470927,\n",
       " 0.9411145024038381,\n",
       " 0.04204197134598394,\n",
       " 0.8900152773398652,\n",
       " 0.017793078605583615,\n",
       " 0.9957427133113841,\n",
       " 0.9984442304640627,\n",
       " 0.9937668212156922,\n",
       " 0.9064467880186894,\n",
       " 0.010117666996689718,\n",
       " 0.8775640891091648,\n",
       " 0.8531192677357166,\n",
       " 0.4941862499124686,\n",
       " 0.9849617946110691,\n",
       " 0.9845349524145037,\n",
       " 0.9990348169698364,\n",
       " 0.9982871936654726,\n",
       " 0.11031275398103646,\n",
       " 0.257234088156064,\n",
       " 0.9990993696996329,\n",
       " 0.9779104657976451,\n",
       " 0.9982768594002243,\n",
       " 0.9883125825886793,\n",
       " 0.9992481689894303,\n",
       " 0.9991464493937231,\n",
       " 0.999465949488634,\n",
       " 0.9965220008867535,\n",
       " 0.999519807087531,\n",
       " 0.9999618169452215,\n",
       " 0.9827413026435169,\n",
       " 0.9973868313863011,\n",
       " 0.9443443395934185,\n",
       " 0.9985160568460354,\n",
       " 0.9942897440559283,\n",
       " 0.9945039503222397,\n",
       " 0.9952533959708434,\n",
       " 0.9867960277159066,\n",
       " 0.9706526658899485,\n",
       " 0.8464314014940891,\n",
       " 0.9988664989608872,\n",
       " 0.9728415926594556,\n",
       " 0.7516170429752546,\n",
       " 0.9999638218714568,\n",
       " 0.8196658502502102,\n",
       " 0.9397423904724962,\n",
       " 0.9926014754468278,\n",
       " 0.7320148802912321,\n",
       " 0.9999846124503586,\n",
       " 0.9937664014273478,\n",
       " 0.961318878095812,\n",
       " 0.9851493893040468,\n",
       " 0.9949824628139927,\n",
       " 0.9997750755934797,\n",
       " 0.6159147330232144,\n",
       " 0.9996068413794643,\n",
       " 0.9989544380458242,\n",
       " 0.9991721640300754,\n",
       " 0.9957513071501408,\n",
       " 0.9995651713429572,\n",
       " 0.9922589620987139,\n",
       " 0.9990959618978703,\n",
       " 0.9975810100386074,\n",
       " 0.9826320106215647,\n",
       " 0.9703908954402163,\n",
       " 0.9993986503291307,\n",
       " 0.9568203350037209,\n",
       " 0.933855819461281,\n",
       " 0.9719811274211976,\n",
       " 0.9884776471080619,\n",
       " 0.9961970667378083,\n",
       " 0.9996765692352757,\n",
       " 0.9945377949978578,\n",
       " 0.016737298415544888,\n",
       " 0.8523079823771205,\n",
       " 0.9577877476583173,\n",
       " 0.9471544442658895,\n",
       " 0.9758982811746505,\n",
       " 0.8027423197671967,\n",
       " 0.9727749992834923,\n",
       " 0.9997174788289845,\n",
       " 0.06563155284593931,\n",
       " 0.6450798145746897,\n",
       " 0.1512859633559844,\n",
       " 0.000747102965886741,\n",
       " 0.011858437977106723,\n",
       " 0.9907199485298107,\n",
       " 0.9979967314092365,\n",
       " 0.9953615671281056,\n",
       " 0.9569244914412067,\n",
       " 0.9663036715661658,\n",
       " 0.9926530835286792,\n",
       " 0.9992140920970246,\n",
       " 0.9973729762821312,\n",
       " 0.9977441767920153,\n",
       " 0.9975958547551828,\n",
       " 0.9988377031573717,\n",
       " 0.9996838718958413,\n",
       " 0.14578249980130376,\n",
       " 0.9951622106824267,\n",
       " 0.9996277225442117,\n",
       " 0.6205844072859963,\n",
       " 0.9995770128872934,\n",
       " 0.5021541714424816,\n",
       " 0.9503089867176697,\n",
       " 0.9944907604869383,\n",
       " 0.997230512066854,\n",
       " 0.7657050274427748,\n",
       " 0.9990066695687638,\n",
       " 0.9557646490740029,\n",
       " 0.9932684910900771,\n",
       " 0.9720362319300632,\n",
       " 0.9988413499148845,\n",
       " 0.993742530566397,\n",
       " 0.9993206953180601,\n",
       " 0.9924288192083066,\n",
       " 0.9986643272056843,\n",
       " 0.9986489341637512,\n",
       " 0.9998662924413283,\n",
       " 0.952219794029068,\n",
       " 0.6023882815018481,\n",
       " 0.9722866964708307,\n",
       " 0.03507181525368549,\n",
       " 0.7848601443881007,\n",
       " 0.10397837335656498,\n",
       " 0.012635309237630572,\n",
       " 0.9911463666712655,\n",
       " 0.9319475180648527,\n",
       " 0.9973430728079911,\n",
       " 0.28827057487562774,\n",
       " 0.05194276300225168,\n",
       " 0.9639187988737036,\n",
       " 0.4821826730726196,\n",
       " 0.9998134880821662,\n",
       " 0.9556111420675091,\n",
       " 0.8826154857356113,\n",
       " 0.9516692276747931,\n",
       " 0.9997452688355781,\n",
       " 0.6476497969859956,\n",
       " 0.9199234000965737,\n",
       " 0.9991976660167049,\n",
       " 0.8509381558392674,\n",
       " 0.930025493964302,\n",
       " 0.9808965880200421,\n",
       " 0.9956763872268628,\n",
       " 0.01933959955861633,\n",
       " 0.9389007462771326,\n",
       " 0.9991330648828849,\n",
       " 0.9934233264392227,\n",
       " 0.9993634009839463,\n",
       " 0.0038962748910491775,\n",
       " 0.8719551391617324,\n",
       " 0.9972657116993985,\n",
       " 0.9942869695643384,\n",
       " 0.4719084135599871,\n",
       " 0.9671650179231999,\n",
       " 0.9899784192039834,\n",
       " 0.9939918565345015,\n",
       " 0.9330611446570162,\n",
       " 0.997987947209626,\n",
       " 0.9974045663029462,\n",
       " 0.8212187657266102,\n",
       " 0.9853952348627333,\n",
       " 0.23713006629670585,\n",
       " 0.9987269804618485,\n",
       " 0.9930724378108823,\n",
       " 0.98350453529875,\n",
       " 0.9967927544497811,\n",
       " 0.957607278795227,\n",
       " 0.981236792210414,\n",
       " 0.9870607389196758,\n",
       " 0.004832017530169345,\n",
       " 0.9353033889829624,\n",
       " 0.9994704082758339,\n",
       " 0.9996673710419102,\n",
       " 0.9958906539914284,\n",
       " 0.999697017541436,\n",
       " 3.6620489989363745e-05,\n",
       " 0.9992447860098675,\n",
       " 0.998483967628607,\n",
       " 0.9861774776392548,\n",
       " 0.9965593545077794,\n",
       " 0.9993592672298275,\n",
       " 0.9996371909767431,\n",
       " 0.9998987631680663,\n",
       " 0.8420690480119088,\n",
       " 0.9995960018969763,\n",
       " 1.938121725095766e-05,\n",
       " 0.8101954937443943,\n",
       " 0.7149897979915271,\n",
       " 0.7866819644475207,\n",
       " 0.950622319810773,\n",
       " 0.9953032975847439,\n",
       " 0.980998583133636,\n",
       " 0.9939299454802443,\n",
       " 0.9821390656836225,\n",
       " 0.9379277660370162,\n",
       " 0.9998759949332472,\n",
       " 0.5133149226128831,\n",
       " 0.9684504663233195,\n",
       " 0.9998241257848608,\n",
       " 0.9933788565543467,\n",
       " 0.9945976089292169,\n",
       " 0.9993062450019268,\n",
       " 0.9677838270568364,\n",
       " 0.14452864825509068,\n",
       " 0.9960240161645002,\n",
       " 0.9984887360769306,\n",
       " 0.9986461662596731,\n",
       " 0.04896135004349637,\n",
       " 0.9838438793228611,\n",
       " 0.9961175813030928,\n",
       " 0.9988406742424133,\n",
       " 0.9980545463141884,\n",
       " 0.9816714609826783,\n",
       " 0.9231514879299308,\n",
       " 0.9755309042428233,\n",
       " 0.5668652726746939,\n",
       " 0.9710142308055183,\n",
       " 0.9961646204985112,\n",
       " 0.9930870727674839,\n",
       " 0.9747114563655486,\n",
       " 0.9959129019812418,\n",
       " 0.9819881511933071,\n",
       " 0.9876614810296105,\n",
       " 0.9387225928627452,\n",
       " 0.018918724545670843,\n",
       " 0.9998866538051774,\n",
       " 0.03716021191230696,\n",
       " 0.7436760421128419,\n",
       " 0.8845593700617412,\n",
       " 0.9714637011443483,\n",
       " 0.9998602327040533,\n",
       " 0.9436123562350769,\n",
       " 0.998621794708622,\n",
       " 0.9998818781375025,\n",
       " 0.9993908207981234,\n",
       " 0.995766033552893,\n",
       " 0.15024162538184727,\n",
       " 0.002562761857810062,\n",
       " 0.7703200752519007,\n",
       " 0.0003345847311947574,\n",
       " 0.999476275642444,\n",
       " 0.9998883824169442,\n",
       " 0.9995515955725593,\n",
       " 0.9194676929404121,\n",
       " 0.4235707628773956,\n",
       " 0.9016815017527401,\n",
       " 0.9133299663638487,\n",
       " 0.5020786730839804,\n",
       " 0.9997233794012468,\n",
       " 0.9982923120029573,\n",
       " 0.9994561211556577,\n",
       " 0.648178190236009,\n",
       " 0.9052195671731149,\n",
       " 0.9864203940564159,\n",
       " 0.8541691696739089,\n",
       " 0.9886081570946256,\n",
       " 0.883062880098231,\n",
       " 0.001311348236149941,\n",
       " 0.9715796689544905,\n",
       " 0.9917377011838431,\n",
       " 0.9998845249136544,\n",
       " 0.9962263796396427,\n",
       " 0.9442852897974382,\n",
       " 0.9561560102281113,\n",
       " 0.9955225973641195,\n",
       " 0.843124111982007,\n",
       " 0.958666108182065,\n",
       " 0.4473364143088009,\n",
       " 0.0010181230456416345,\n",
       " 0.9926564903665741,\n",
       " 0.999982610216001,\n",
       " 0.9989870811640327,\n",
       " 0.975836022424952,\n",
       " 0.9997175364797226,\n",
       " 0.8318736692293428,\n",
       " 0.9462096999520307,\n",
       " 0.998636945185341,\n",
       " 0.965150339565608,\n",
       " 0.9871675630260792,\n",
       " 0.9974682213402376,\n",
       " 0.8168164196266332,\n",
       " 0.29869263109025446,\n",
       " 0.9482191739431757,\n",
       " 0.0005132421853496301,\n",
       " 0.9983264155171427,\n",
       " 0.9956587218405908,\n",
       " 0.9837371032810799,\n",
       " 0.945719679717543,\n",
       " 8.520356871947506e-08,\n",
       " 0.9934176948011475,\n",
       " 0.9927080355917509,\n",
       " 0.9912349283176761,\n",
       " 0.9687743266812658,\n",
       " 0.016894701445088853,\n",
       " 0.9742999170464481,\n",
       " 0.9832069955699687,\n",
       " 0.9984710907923124,\n",
       " 0.35724037331657915,\n",
       " 0.9346224734826819,\n",
       " 0.0043084665679091775,\n",
       " 0.9007051763218372,\n",
       " 0.9733297830304164,\n",
       " 0.7798621492870056,\n",
       " 0.9662322936434206,\n",
       " 0.9944262305897399,\n",
       " 0.9998978847964587,\n",
       " 0.9862707510807497,\n",
       " 0.9997850894228676,\n",
       " 0.9735468676129647,\n",
       " 0.9899345602027936,\n",
       " 0.2778300372330659,\n",
       " 0.5864624273938759,\n",
       " 0.997720697762576,\n",
       " 0.9944306177125325,\n",
       " 0.9989394027311036,\n",
       " 0.9892222980855944,\n",
       " 0.00044161613990921565,\n",
       " 0.9967739037011559,\n",
       " 0.883734458031163,\n",
       " 0.9933563298792427,\n",
       " 0.44644360959114227,\n",
       " 0.9878013811901003,\n",
       " 0.9930095603370119,\n",
       " 0.999910734603956,\n",
       " 0.9997699339904071,\n",
       " 0.997063623266819,\n",
       " 0.017669778838540852,\n",
       " 0.9995312221037254,\n",
       " 0.9865409397802449,\n",
       " 0.9845979834619798,\n",
       " 0.8427623832534379,\n",
       " 0.6480739307728042,\n",
       " 0.7079539047928921,\n",
       " 0.994918149737424,\n",
       " 0.9999880915180777,\n",
       " 0.9997481899396654,\n",
       " 0.9442786912977363,\n",
       " 0.99905231395064,\n",
       " 0.9980499981580396,\n",
       " 0.9986906749739448,\n",
       " 0.9950542042684933,\n",
       " 0.996995817476444,\n",
       " 0.9674371610007811,\n",
       " 0.9305693610798721,\n",
       " 0.9965539256242448,\n",
       " 0.9890763595610829,\n",
       " 0.8471373715729508,\n",
       " 0.8238041901337176,\n",
       " 0.9112881560169754,\n",
       " 0.9984579497826049,\n",
       " 0.9661698918000039,\n",
       " 0.9859768377728513,\n",
       " 0.9735852286682426,\n",
       " 0.9990283105642628,\n",
       " 0.994017887839317,\n",
       " 0.8934656014842959,\n",
       " 0.8205414857994665,\n",
       " 0.02328955206230707,\n",
       " 0.39002970941581855,\n",
       " 0.9754370842778091,\n",
       " 0.08851796297867884,\n",
       " 0.8674507063326453,\n",
       " 4.167303678739542e-05,\n",
       " 0.00018227166491520156,\n",
       " 0.2806236456467365,\n",
       " 0.27327711790952464,\n",
       " 0.9195799430434878,\n",
       " 0.8166073195847836,\n",
       " 0.9690384399452889,\n",
       " 0.999246644942747,\n",
       " 0.6622598819887867,\n",
       " 0.9883086408823231,\n",
       " 0.8725516370103085,\n",
       " 0.9004965863985059,\n",
       " 0.18877227685414316,\n",
       " 2.1617102874225302e-05,\n",
       " 0.9962252420934092,\n",
       " 0.9801451796977343,\n",
       " 0.07020257328384474,\n",
       " 0.9934413442921807,\n",
       " 0.9972451910404048,\n",
       " 0.9999696524258571,\n",
       " 0.9667524302111099,\n",
       " 0.8224689820504651,\n",
       " 0.9917539060060736,\n",
       " 0.023432194353456587,\n",
       " 0.9986813900492053,\n",
       " 0.9993127652201275,\n",
       " 0.9931372678433028,\n",
       " 0.9980371079433482,\n",
       " 0.9874999717885496,\n",
       " 0.8493169401342371,\n",
       " 0.9884646560896001,\n",
       " 0.9759361723483571,\n",
       " 0.9390882920154529,\n",
       " 0.9969070495321148,\n",
       " 0.011290207296059866,\n",
       " 0.9913931600050546,\n",
       " 2.841863437581319e-06,\n",
       " 3.45031358062236e-06,\n",
       " 0.9996279246680313,\n",
       " 0.9979978923912604,\n",
       " 0.8317550584676937,\n",
       " 0.9785580517921053,\n",
       " 0.4447614652316411,\n",
       " 0.9989216016351777,\n",
       " 0.9988276734010325,\n",
       " 0.9983779770313724,\n",
       " 0.9989297741600466,\n",
       " 0.9509198973914705,\n",
       " 0.17656639118834397,\n",
       " 0.6047970912161368,\n",
       " 0.5548512090077623,\n",
       " 0.9997794769731255,\n",
       " 0.9965474237202755,\n",
       " 0.9995037123938539,\n",
       " 0.9916194621740225,\n",
       " 0.9980999667728143,\n",
       " 0.989218860440675,\n",
       " 0.7223609996089195,\n",
       " 0.9973309583939572,\n",
       " 0.9999628782447885,\n",
       " 0.9925774115567119,\n",
       " 0.9999721823403277,\n",
       " 0.9998706218240933,\n",
       " 0.9998540794000645,\n",
       " 0.9995457848206456,\n",
       " 0.032578833808438734,\n",
       " 0.934394554688111,\n",
       " 0.9994577081574524,\n",
       " 0.9543889259499032,\n",
       " 0.9434874284476945,\n",
       " 0.995446684297014,\n",
       " 0.9808688504035866,\n",
       " 0.9994326998125906,\n",
       " 0.998806757248288,\n",
       " 0.9739796828180971,\n",
       " 6.541560081172528e-05,\n",
       " 0.9897772088588752,\n",
       " 0.00022856989107188785,\n",
       " 0.9740446544866187,\n",
       " 0.9862829820475038,\n",
       " 0.9987247185110366,\n",
       " 0.9649883485288858,\n",
       " 0.9853938678713844,\n",
       " 0.14079375281136067,\n",
       " 0.08075555001134051,\n",
       " 0.9938170127969495,\n",
       " 0.0002881265056986931,\n",
       " 0.05799070445968224,\n",
       " 0.5656271568275207,\n",
       " 0.9422650257772249,\n",
       " 0.9071092388364352,\n",
       " 0.9892693655725501,\n",
       " 0.9041790428430652,\n",
       " 0.9949047261957813,\n",
       " 0.9999565596319364,\n",
       " ...]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob_all = model.predict_proba(X)\n",
    "list(y_prob_all[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06262643706517985"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_prob_all[y]<0.01) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2\n",
    "### Network 1\n",
    "-  Create neural network with single hidden layer\n",
    "-  Use **Accuracy** as metric of performance  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['    def base_model(self, nodes, num_output=31, kernel_regularizer=None, layer_id=None):\\n',\n",
       "  '        model = Sequential()\\n',\n",
       "  '        for prev_node, node in zip(nodes[:-1], nodes[1:]):\\n',\n",
       "  '            layer_name = None\\n',\n",
       "  '            if layer_id is not None:\\n',\n",
       "  '                layer_name = \"{}-in-{}-n-{}\".format(layer_id, str(prev_node), str(prev_node))\\n',\n",
       "  \"            model.add(Dense(node, activation='relu', kernel_regularizer=kernel_regularizer,\\n\",\n",
       "  '                            input_dim=prev_node, name=layer_name))\\n',\n",
       "  \"        model.add(Dense(num_output, activation='sigmoid', name='features'))\\n\",\n",
       "  \"        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\\n\",\n",
       "  '        return model\\n'],\n",
       " 38)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(nn.train)\n",
    "inspect.getsourcelines(nn.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "811/811 [==============================] - 0s 49us/step\n",
      "Test loss: 0.07202654960306887\n",
      "Test accuracy: 0.9743049353471372\n"
     ]
    }
   ],
   "source": [
    "nn.train(X, y, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    ">  With just single layer we are getting accuracy of 97% Epoch=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Step 3\n",
    "### Network 2\n",
    "-  Use the same model as step 1 for neurons  = 5 to 32\n",
    "-  Measure accuracy at each step to find optimal number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['    def train_network_2(self, X, y):\\n',\n",
       "  '        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=5)\\n',\n",
       "  '        for P in range(2, 12):\\n',\n",
       "  '            for num_layers in range(1, 2):\\n',\n",
       "  '                nodes = [64] + [int(P / 2)] * num_layers\\n',\n",
       "  '                print(nodes)\\n',\n",
       "  '                model = self.base_model(nodes)\\n',\n",
       "  '                summary = model.fit(X_train, y_train, epochs=10, verbose=0)\\n',\n",
       "  '                score = model.evaluate(X_test, y_test)\\n',\n",
       "  \"                print('Test loss:', score[0])\\n\",\n",
       "  \"                print('Test accuracy:', score[1])\\n\"],\n",
       " 112)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(nn.train_network_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 1]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                62        \n",
      "=================================================================\n",
      "Total params: 127\n",
      "Trainable params: 127\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 155us/step\n",
      "Test loss: 0.12151120850089506\n",
      "Test accuracy: 0.9692696872519694\n",
      "[64, 1]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                62        \n",
      "=================================================================\n",
      "Total params: 127\n",
      "Trainable params: 127\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 163us/step\n",
      "Test loss: 0.11839029295843224\n",
      "Test accuracy: 0.9677021204984182\n",
      "[64, 2]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                93        \n",
      "=================================================================\n",
      "Total params: 223\n",
      "Trainable params: 223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 175us/step\n",
      "Test loss: 0.10982383156519661\n",
      "Test accuracy: 0.9703120669238839\n",
      "[64, 2]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                93        \n",
      "=================================================================\n",
      "Total params: 223\n",
      "Trainable params: 223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 172us/step\n",
      "Test loss: 0.1106225375176123\n",
      "Test accuracy: 0.9702086245478464\n",
      "[64, 3]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                124       \n",
      "=================================================================\n",
      "Total params: 319\n",
      "Trainable params: 319\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 186us/step\n",
      "Test loss: 0.09182437076304543\n",
      "Test accuracy: 0.9706224006387949\n",
      "[64, 3]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                124       \n",
      "=================================================================\n",
      "Total params: 319\n",
      "Trainable params: 319\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 186us/step\n",
      "Test loss: 0.09360917378998061\n",
      "Test accuracy: 0.9702563795422355\n",
      "[64, 4]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                155       \n",
      "=================================================================\n",
      "Total params: 415\n",
      "Trainable params: 415\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 196us/step\n",
      "Test loss: 0.08343470053950378\n",
      "Test accuracy: 0.9729140582317546\n",
      "[64, 4]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                155       \n",
      "=================================================================\n",
      "Total params: 415\n",
      "Trainable params: 415\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 206us/step\n",
      "Test loss: 0.08388116429401374\n",
      "Test accuracy: 0.9720228620509415\n",
      "[64, 5]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                186       \n",
      "=================================================================\n",
      "Total params: 511\n",
      "Trainable params: 511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 211us/step\n",
      "Test loss: 0.07306450262820127\n",
      "Test accuracy: 0.975523989601681\n",
      "[64, 5]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                186       \n",
      "=================================================================\n",
      "Total params: 511\n",
      "Trainable params: 511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 218us/step\n",
      "Test loss: 0.07539498982799424\n",
      "Test accuracy: 0.9743065516467628\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def base_model(nodes, num_output=31, kernel_regularizer=None, layer_id=None):\n",
    "    model = Sequential()\n",
    "    layer_name = None\n",
    "    for prev_node, node in zip(nodes[:-1], nodes[1:]):\n",
    "        layer_name = \"{}-in-{}-n-{}\".format(layer_id, str(prev_node), str(prev_node))\n",
    "        model.add(Dense(node, activation='relu', kernel_regularizer=kernel_regularizer,\n",
    "        input_dim=prev_node, name=layer_name))\n",
    "    model.add(Dense(num_output, activation='sigmoid', name='features'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_network_debug(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=5)\n",
    "    for P in range(2, 12):\n",
    "        for num_layers in range(1, 2):\n",
    "            nodes = [64] + [int(P / 2)] * num_layers\n",
    "            print(nodes)\n",
    "            model = base_model(nodes)\n",
    "            print(model.summary())\n",
    "            summary = model.fit(X_train, y_train, epochs=10, verbose=0)\n",
    "            score = model.evaluate(X_test, y_test)\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "\n",
    "train_network_debug(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "nn.train_network_2(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    ">  ### Accuraccy is already at 96% in step 1, so we add regularize function to decrease accuracy and determine optimal number of neurons as 6\n",
    ">  ### P=6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Step 4\n",
    "### Network 3\n",
    "-  Find optimal number of hidden layers with number of neurons as P/2\n",
    "-  Again, since the accuracy is very high, we will have to take regularizer function\n",
    "-  P=6, Number of neurons = P/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['    def train_network_3(self, X, y):\\n',\n",
       "  '        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=5)\\n',\n",
       "  '        P = 6\\n',\n",
       "  '        for num_layers in range(1, 10):\\n',\n",
       "  '            nodes = [64] + [int(P / 2)] * num_layers\\n',\n",
       "  '            model = self.base_model(nodes)\\n',\n",
       "  '            summary = model.fit(X_train, y_train, epochs=10, verbose=0)\\n',\n",
       "  '            score = model.evaluate(X_test, y_test)\\n',\n",
       "  \"            print('Test loss:', score[0])\\n\",\n",
       "  \"            print('Test accuracy:', score[1])\\n\"],\n",
       " 101)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(nn.train_network_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4054/4054 [==============================] - 0s 58us/step\n",
      "Test loss: 0.09179627784717595\n",
      "Test accuracy: 0.97132263212411\n",
      "4054/4054 [==============================] - 0s 62us/step\n",
      "Test loss: 0.08782961113689919\n",
      "Test accuracy: 0.9706621874890189\n",
      "4054/4054 [==============================] - 0s 69us/step\n",
      "Test loss: 0.11377906092117734\n",
      "Test accuracy: 0.9698903405672232\n",
      "4054/4054 [==============================] - 0s 77us/step\n",
      "Test loss: 0.12015273502499939\n",
      "Test accuracy: 0.9702404533693457\n",
      "4054/4054 [==============================] - 0s 82us/step\n",
      "Test loss: 0.13819008715704384\n",
      "Test accuracy: 0.9677419066429138\n",
      "4054/4054 [==============================] - 0s 93us/step\n",
      "Test loss: 0.1198273547908974\n",
      "Test accuracy: 0.9702165809298714\n",
      "4054/4054 [==============================] - 0s 104us/step\n",
      "Test loss: 0.121142026733517\n",
      "Test accuracy: 0.9700574382338315\n",
      "4054/4054 [==============================] - 0s 122us/step\n",
      "Test loss: 0.1381859623068474\n",
      "Test accuracy: 0.9677419066429138\n",
      "4054/4054 [==============================] - 1s 133us/step\n",
      "Test loss: 0.11945707400590842\n",
      "Test accuracy: 0.9698028180125199\n"
     ]
    }
   ],
   "source": [
    "nn.train_network_3(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    ">  ### Because of high accuracy, we can select number of layers=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Step 5\n",
    "### Network 4\n",
    "-  Without any conditions, determine optimal architecture\n",
    "-  We can run the training module with P=6 and Layers=6\n",
    "-  Save model for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['    def train_with_callback(self, X, y, model_file: str = None, generate_plots=True):\\n',\n",
       "  '        if model_file is None:\\n',\n",
       "  '            model_file = self._model\\n',\n",
       "  '        plot_losses = TrainingPlot()\\n',\n",
       "  '        time_summary = TimeSummary()\\n',\n",
       "  \"        layer_names = ['features']\\n\",\n",
       "  '        num_nodes = 6\\n',\n",
       "  '        for layer_id in range(6, 7):\\n',\n",
       "  '            nodes = [64, num_nodes]\\n',\n",
       "  '            for prev_node, node in zip(nodes[:-1], nodes[1:]):\\n',\n",
       "  '                layer_name = None\\n',\n",
       "  '                if layer_id is not None:\\n',\n",
       "  '                    layer_name = \"{}-in-{}-n-{}\".format(layer_id, str(prev_node), str(prev_node))\\n',\n",
       "  '                    layer_names.append(layer_name)\\n',\n",
       "  '\\n',\n",
       "  '        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=5)\\n',\n",
       "  \"        with open(os.path.join(self._log_dir, 'metadata.tsv'), 'w') as f:\\n\",\n",
       "  '            np.savetxt(f, y_test)\\n',\n",
       "  '        tensorboard = TensorBoard(log_dir=self._log_dir, histogram_freq=2, batch_size=32,\\n',\n",
       "  '                                  write_graph=True,\\n',\n",
       "  '                                  write_grads=False, write_images=True, embeddings_freq=1,\\n',\n",
       "  \"                                  embeddings_layer_names=['features'],\\n\",\n",
       "  '                                  embeddings_data=X_test,\\n',\n",
       "  \"                                  update_freq='epoch')\\n\",\n",
       "  '        callbacks = [tensorboard, time_summary]\\n',\n",
       "  '        if generate_plots:\\n',\n",
       "  '            callbacks.append(plot_losses)\\n',\n",
       "  '        for layer in range(6, 7):\\n',\n",
       "  '            nodes = [64, num_nodes]\\n',\n",
       "  '            model = self.base_model(nodes, layer_id=\"l-{}\".format(layer))\\n',\n",
       "  '            summary = model.fit(X_train, y_train, epochs=100, verbose=0, validation_data=(X_test, y_test),\\n',\n",
       "  '                                callbacks=callbacks)\\n',\n",
       "  '            score = model.evaluate(X_test, y_test)\\n',\n",
       "  '            if generate_plots:\\n',\n",
       "  '                plot_training_summary(summary, time_summary)\\n',\n",
       "  '            score = model.evaluate(X_test, y_test)\\n',\n",
       "  '            model.save(model_file)\\n',\n",
       "  \"            print('Test loss:', score[0])\\n\",\n",
       "  \"            print('Test accuracy:', score[1])\\n\"],\n",
       " 61)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(nn.train_with_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-786875b5b4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/tf/models/model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tf/notebooks/NNKeras.py\u001b[0m in \u001b[0;36mtrain_with_callback\u001b[0;34m(self, X, y, model_file, generate_plots)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"l-{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             summary = model.fit(X_train, y_train, epochs=100, verbose=0, validation_data=(X_test, y_test),\n\u001b[0;32m---> 92\u001b[0;31m                                 callbacks=callbacks)\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgenerate_plots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    979\u001b[0m                                     os.path.join(self.log_dir,\n\u001b[1;32m    980\u001b[0m                                                  'keras_embedding.ckpt'),\n\u001b[0;32m--> 981\u001b[0;31m                                     epoch)\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m           self.export_meta_graph(\n\u001b[0;32m-> 1196\u001b[0;31m               meta_graph_filename, strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m         strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m       \u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1561\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1562\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, saver_def, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscoped_meta_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     71\u001b[0m                                         text_format.MessageToString(graph_def))\n\u001b[1;32m     72\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn.train_with_callback(X, y, '/tf/models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "### In this dataset, our neural network reaches accuracy of 96% with sample data and 98% with full dataset. Hence it is necessary to validate the accuracy of data with trained model.\n",
    "### Keras allows callbacks for training visualization. Function below runs for 200 epochs with 32 neurons and saves the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Predict from souce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inspect.getsourcelines(nn.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nn.predict(\"/tf/dataset/ae002161.csv\", unique_classes, 5, \"/tf/models/model_full.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nn.predict(\"/tf/dataset/ae003852.csv\", unique_classes, 5, \"/tf/models/model_full.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    ">  ### As we can see that the accuracy is as expected for the first case but for second, it is less than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Step 6\n",
    "### Network 5\n",
    "-  We will use same training function with each column of y as 1D matrix\n",
    "-  Take average of each inidividual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "inspect.getsourcelines(nn.single_output_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "avg_score = nn.single_output_score(X, y)\n",
    "avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    ">  ### Average accuracy of individual prediction is less than overall prediction accuracy. Hence, we can conclude that it is better to create one network with N output neurons than N networks each with one output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "-  ### Generic module for generating optimized network\n",
    "-  ### Visualize t-SNE for hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation with linear classification methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "##############  Validation with linear classification methods\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "tmp = DecisionTreeClassifier(min_samples_leaf=10)\n",
    "tmp.fit(X_train, y_train)\n",
    "y_pred = tmp.predict(X_test)\n",
    "print('test', accuracy_score(y_pred, y_test))\n",
    "y_pred_train = tmp.predict(X_train)\n",
    "print('train', accuracy_score(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "##############  Validation with linear classification methods\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train)\n",
    "# print(pca.components_)\n",
    "X_proj = pca.transform(X_test)\n",
    "\n",
    "f, ax = plt.subplots(2, sharex=True)\n",
    "f.set_figheight(10)\n",
    "ax[0].scatter(X_proj[:, 0], X_proj[:, 1], c=np.argmax(y_test, axis=1), alpha=0.9)\n",
    "ax[1].scatter(X_proj[:, 0], X_proj[:, 1], c=np.argmax(y_pred, axis=1), alpha=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
