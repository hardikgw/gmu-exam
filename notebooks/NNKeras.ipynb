{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.5/dist-packages (1.16.2)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.5/dist-packages (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (1.16.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.5/dist-packages (from pandas) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.5/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.5/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.5/dist-packages (from sklearn) (0.21.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.5/dist-packages (from scikit-learn->sklearn) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.5/dist-packages (2.2.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from keras) (5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from keras) (1.2.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras) (2.9.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.5/dist-packages (3.0.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (1.0.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from matplotlib) (1.16.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from kiwisolver>=1.0.1->matplotlib) (40.8.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "\u001b[33mYou are using pip version 19.0.3, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install pandas\n",
    "!{sys.executable} -m pip install sklearn\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "from utils import TrainingPlot, TimeSummary, plot_training_summary\n",
    "from NNKeras import NNKeras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import inspect\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Step 1\n",
    "### Data Preparation\n",
    "\n",
    "#### Data Cleanup\n",
    "-  Merge 64 size vectors by ignoring line breaks\n",
    "-  Create subset of data by selecting non consecutive vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "#### Cleaned data\n",
    "L42023,0.04347826,0.04347826,0.,0.04347826,0.01086957,0.02173913,0.,0.02173913,0.,0.,0.,0.,0.,0.02173913,0.02173913,0.04347826,0.07608696,0.02173913,0.,0.0326087,0.01086957,0.,0.,0.0326087,0.,0.01086957,0.,0.0326087,0.,0.,0.,0.0326087,0.05434783,0.,0.01086957,0.02173913,0.04347826,0.,0.01086957,0.02173913,0.02173913,0.,0.,0.01086957,0.0326087,0.,0.04347826,0.0326087,0.01086957,0.01086957,0.,0.02173913,0.04347826,0.01086957,0.,0.01086957,0.,0.,0.,0.,0.04347826,0.02173913,0.,0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "## Network 1\n",
    "### Create input dataset X and y where X has all vectors and y is one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "nn = NNKeras(\"/tf/dataset/dataset_full_rnd.csv\")\n",
    "X, y, unique_classes = nn.read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Vector Shape\n",
      "(81068, 64)\n",
      "Initialized Training Vector Shape\n",
      "(81068, 31)\n",
      "Input Vector Head\n",
      "         1         2         3         4         5         6         7   \\\n",
      "0  0.036508  0.019048  0.052381  0.023810  0.003175  0.011111  0.004762   \n",
      "1  0.060345  0.017241  0.000000  0.017241  0.025862  0.017241  0.000000   \n",
      "2  0.015337  0.006135  0.015337  0.015337  0.000000  0.006135  0.018405   \n",
      "\n",
      "         8         9         10  ...        55        56   57        58  \\\n",
      "0  0.007937  0.015873  0.007937  ...  0.001587  0.003175  0.0  0.001587   \n",
      "1  0.034483  0.000000  0.008621  ...  0.000000  0.008621  0.0  0.025862   \n",
      "2  0.009202  0.003067  0.012270  ...  0.024540  0.003067  0.0  0.003067   \n",
      "\n",
      "         59   60        61        62        63        64  \n",
      "0  0.014286  0.0  0.019048  0.011111  0.014286  0.028571  \n",
      "1  0.000000  0.0  0.008621  0.034483  0.025862  0.034483  \n",
      "2  0.006135  0.0  0.003067  0.000000  0.030675  0.012270  \n",
      "\n",
      "[3 rows x 64 columns]\n",
      "Training Vector Head\n",
      "[[ True False False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False]\n",
      " [False  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False]\n",
      " [False False  True False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False]]\n",
      "Categories\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NC_000961</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AE005672</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL450380</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL645882</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL009126</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AB001339</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AE014075</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AE000782</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AE009442</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>L42023</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CP000025</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AE000666</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>AE003853</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NC_00918</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AE000512</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AE002098</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>AE004092</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BA000002</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>AE004091</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>AE003852</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>BA000004</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>AL139299</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AE006470</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>AE000516</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>AE000520</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>AE001437</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>AE015924</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>L77117</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>CP000241</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>AE002161</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>AL096836</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0  indices\n",
       "0   NC_000961        1\n",
       "1    AE005672        2\n",
       "2    AL450380        3\n",
       "3    AL645882        4\n",
       "4    AL009126        5\n",
       "5    AB001339        6\n",
       "6    AE014075        7\n",
       "7    AE000782        8\n",
       "8    AE009442        9\n",
       "9      L42023       10\n",
       "10   CP000025       11\n",
       "11   AE000666       12\n",
       "12   AE003853       13\n",
       "13   NC_00918       14\n",
       "14   AE000512       15\n",
       "15   AE002098       16\n",
       "16   AE004092       17\n",
       "17   BA000002       18\n",
       "18   AE004091       19\n",
       "19   AE003852       20\n",
       "20   BA000004       21\n",
       "21   AL139299       22\n",
       "22   AE006470       23\n",
       "23   AE000516       24\n",
       "24   AE000520       25\n",
       "25   AE001437       26\n",
       "26   AE015924       27\n",
       "27     L77117       28\n",
       "28   CP000241       29\n",
       "29   AE002161       30\n",
       "30   AL096836       31"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Input Vector Shape\")\n",
    "print(X.shape)\n",
    "print(\"Initialized Training Vector Shape\")\n",
    "print(y.shape)\n",
    "print(\"Input Vector Head\")\n",
    "print(X[:3])\n",
    "print(\"Training Vector Head\")\n",
    "print(y[:3])\n",
    "print(\"Categories\")\n",
    "unique_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "### Prepare Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             1         2         3         4         5         6         7   \\\n",
      "61310  0.059006  0.015528  0.040373  0.040373  0.012422  0.003106  0.015528   \n",
      "28317  0.031579  0.024561  0.014035  0.010526  0.014035  0.007018  0.017544   \n",
      "48590  0.001818  0.010909  0.010909  0.003636  0.000000  0.016364  0.009091   \n",
      "\n",
      "             8         9         10  ...        55        56        57  \\\n",
      "61310  0.006211  0.015528  0.006211  ...  0.003106  0.009317  0.003106   \n",
      "28317  0.010526  0.000000  0.007018  ...  0.007018  0.010526  0.000000   \n",
      "48590  0.000000  0.000000  0.036364  ...  0.005455  0.000000  0.001818   \n",
      "\n",
      "             58        59        60        61        62        63        64  \n",
      "61310  0.003106  0.018634  0.006211  0.006211  0.021739  0.031056  0.037267  \n",
      "28317  0.000000  0.010526  0.000000  0.038596  0.010526  0.007018  0.017544  \n",
      "48590  0.016364  0.030909  0.000000  0.000000  0.020000  0.018182  0.001818  \n",
      "\n",
      "[3 rows x 64 columns]\n",
      "[[False False False False False False False  True False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False False False  True False False False\n",
      "  False False False False False False False]\n",
      " [False False False False False False False False False False False False\n",
      "  False False False False False False  True False False False False False\n",
      "  False False False False False False False]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "print(X_train[:3])\n",
    "print(y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model = SVR(kernel=\"linear\")\n",
    "model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-efce737d9ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and multiclass targets"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred, np.argmax(y_test, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test acc 0.8078820772172197\n",
      "train acc 0.8074752521047276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(X_train, np.argmax(y_train, axis=1))\n",
    "y_pred = model.predict(X_test)\n",
    "print('test acc', accuracy_score(y_pred, np.argmax(y_test, axis=1)))\n",
    "y_pred_train = model.predict(X_train)\n",
    "print('train acc', accuracy_score(y_pred_train, np.argmax(y_train, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8533502144407433,\n",
       " 0.8043424250038147,\n",
       " 0.3066285127740533,\n",
       " 0.20473939648892167,\n",
       " 0.9918417669533364,\n",
       " 0.9942873781563674,\n",
       " 0.9999993905078743,\n",
       " 0.00018587655304635147,\n",
       " 0.9960794574113141,\n",
       " 0.016325890979279077,\n",
       " 0.8646697410080402,\n",
       " 0.9899644196411276,\n",
       " 0.9999600656975288,\n",
       " 0.9922537488058185,\n",
       " 0.9011494856094303,\n",
       " 0.13133728018944124,\n",
       " 0.9998188035018369,\n",
       " 0.999944471791052,\n",
       " 0.02165085013834678,\n",
       " 0.9999761943553935,\n",
       " 0.9952347502100911,\n",
       " 0.9994010027528971,\n",
       " 0.3194306882932444,\n",
       " 0.7934040089769205,\n",
       " 0.99988504498048,\n",
       " 0.9999985478111811,\n",
       " 0.9998903902284343,\n",
       " 0.007711920538009108,\n",
       " 0.9988477135065463,\n",
       " 0.6405078553580968,\n",
       " 0.9986280120254297,\n",
       " 0.9987045902239635,\n",
       " 0.18372294331372743,\n",
       " 0.9257020648813554,\n",
       " 1.7882074455912956e-11,\n",
       " 0.719248304806497,\n",
       " 0.9998105838796392,\n",
       " 0.999999779662127,\n",
       " 0.8278414756655972,\n",
       " 0.8598025954455121,\n",
       " 0.08923062865527148,\n",
       " 0.4662514457977978,\n",
       " 0.3425059296417812,\n",
       " 0.9997819178618449,\n",
       " 0.9998142305419275,\n",
       " 0.9929775821230262,\n",
       " 0.9129084650861109,\n",
       " 0.8881152904066898,\n",
       " 0.9999999791401217,\n",
       " 0.9999883287994896,\n",
       " 0.9866386200048682,\n",
       " 0.9761439216037424,\n",
       " 0.9999743160643692,\n",
       " 0.9999945748436632,\n",
       " 0.9493196715743719,\n",
       " 0.9732590133473031,\n",
       " 0.999946711346769,\n",
       " 0.6730129940922894,\n",
       " 0.9245241743505656,\n",
       " 0.0347333113268512,\n",
       " 0.9999838365970851,\n",
       " 0.9791178408609199,\n",
       " 0.03575383902825709,\n",
       " 0.9999643396749046,\n",
       " 0.9948943801744742,\n",
       " 0.9999999369597126,\n",
       " 0.4176825845181031,\n",
       " 0.0208274696082013,\n",
       " 0.9659080444307551,\n",
       " 0.9749356028106307,\n",
       " 0.9983044241480951,\n",
       " 0.9998981152738948,\n",
       " 0.9999989647075287,\n",
       " 0.9991079243373372,\n",
       " 0.991107252348428,\n",
       " 0.0001413500717966746,\n",
       " 0.23331961688692954,\n",
       " 0.0725780032669386,\n",
       " 0.9934216007130372,\n",
       " 0.9865535486522409,\n",
       " 0.9982462225108194,\n",
       " 0.9999021451483887,\n",
       " 0.7075112249192637,\n",
       " 0.7841124157193133,\n",
       " 0.7239356959406549,\n",
       " 0.9999401568006879,\n",
       " 0.725128426991141,\n",
       " 0.9535047692739689,\n",
       " 0.9998945440126424,\n",
       " 0.8354854273168638,\n",
       " 0.8796136362437688,\n",
       " 0.7759379202184057,\n",
       " 0.9717892744337963,\n",
       " 0.15558533391038776,\n",
       " 0.9607319435036389,\n",
       " 0.999999811453909,\n",
       " 0.9999946975872438,\n",
       " 0.999809687422683,\n",
       " 0.5486663188643472,\n",
       " 0.9996191102871487,\n",
       " 0.9978317617738204,\n",
       " 0.5162168808876004,\n",
       " 0.32051672151172167,\n",
       " 0.44166357605544193,\n",
       " 0.9999384000389508,\n",
       " 0.999999623906519,\n",
       " 0.9972581544090382,\n",
       " 0.9995511660409717,\n",
       " 0.7584934688872581,\n",
       " 0.10718545849497511,\n",
       " 6.715868219754948e-05,\n",
       " 0.989848777939784,\n",
       " 0.9827352426151659,\n",
       " 0.8827942367318566,\n",
       " 0.9999998956918474,\n",
       " 0.9999459582844661,\n",
       " 0.9300161093514606,\n",
       " 0.9917920690756197,\n",
       " 0.1731075720168471,\n",
       " 0.012747743743821472,\n",
       " 0.9488152253024627,\n",
       " 0.9999999995004523,\n",
       " 0.49266614359820937,\n",
       " 3.338517934684565e-05,\n",
       " 0.9963933605983654,\n",
       " 0.16686032174859605,\n",
       " 0.9996758218554821,\n",
       " 0.11024323066573909,\n",
       " 0.8119264209381142,\n",
       " 0.8977190801068946,\n",
       " 0.9843408854277692,\n",
       " 2.288340578018196e-05,\n",
       " 0.9998258496095753,\n",
       " 0.9987922646916474,\n",
       " 0.9796139517174102,\n",
       " 0.25847972408746966,\n",
       " 0.9999989344488428,\n",
       " 0.8026638994967072,\n",
       " 0.352685286418071,\n",
       " 0.3841725884989336,\n",
       " 0.0002881419744449133,\n",
       " 0.008888873002216626,\n",
       " 1.7118531086157407e-05,\n",
       " 0.9993224099096883,\n",
       " 0.9999992858365694,\n",
       " 0.9996504852143204,\n",
       " 0.9991918809632218,\n",
       " 0.9912917647841419,\n",
       " 0.9974467872168375,\n",
       " 0.9991520387503576,\n",
       " 0.9915873842585415,\n",
       " 0.999833939857544,\n",
       " 0.9999995521336699,\n",
       " 0.7836038950249111,\n",
       " 0.9991247142482418,\n",
       " 0.9967001250509792,\n",
       " 0.8402750309674627,\n",
       " 0.9999726939982007,\n",
       " 0.9628380138988988,\n",
       " 0.9934970851474911,\n",
       " 0.5438352012282921,\n",
       " 0.9997886044270438,\n",
       " 0.7392623513055315,\n",
       " 0.9990788279461413,\n",
       " 0.9999317496167548,\n",
       " 0.7771261996126357,\n",
       " 0.9432439001034539,\n",
       " 0.07850677234619663,\n",
       " 3.2854769345326974e-06,\n",
       " 0.6307861029703566,\n",
       " 0.9960749086725131,\n",
       " 0.9999983685087969,\n",
       " 0.9773675518854829,\n",
       " 0.9999142036620144,\n",
       " 0.9395187661557175,\n",
       " 0.9999994390861093,\n",
       " 0.9990091227925751,\n",
       " 0.9999970639855229,\n",
       " 0.9998596211880373,\n",
       " 0.007239104606664282,\n",
       " 0.08036114560551096,\n",
       " 0.9139098718166958,\n",
       " 0.13461827014596595,\n",
       " 0.9999978199807863,\n",
       " 0.7102268928059767,\n",
       " 0.7835460651248407,\n",
       " 6.755456511092383e-05,\n",
       " 0.40882463984014444,\n",
       " 0.9998638341907897,\n",
       " 0.9978914231712659,\n",
       " 0.9993422759167881,\n",
       " 0.9906338869048922,\n",
       " 0.9999940137315463,\n",
       " 0.9994718990448596,\n",
       " 0.17397660527613956,\n",
       " 0.7999089723770477,\n",
       " 0.9641944215730204,\n",
       " 0.8403623862469789,\n",
       " 0.9945735166923894,\n",
       " 0.998538713667488,\n",
       " 0.8215507136674904,\n",
       " 0.004827165570626045,\n",
       " 0.8517326349965791,\n",
       " 0.07138904231484322,\n",
       " 0.9969882136535827,\n",
       " 0.9999953794252197,\n",
       " 0.9998022145070591,\n",
       " 0.9999060121883006,\n",
       " 0.9986539494860718,\n",
       " 0.5472751880873966,\n",
       " 0.8694516297733463,\n",
       " 0.9999002181051885,\n",
       " 0.9999970501722231,\n",
       " 0.9999942193814657,\n",
       " 0.01064565608149432,\n",
       " 0.9999319119627166,\n",
       " 0.9999999907040812,\n",
       " 0.1724706649982184,\n",
       " 3.436716655457921e-06,\n",
       " 0.9985721109240175,\n",
       " 0.9550596655344226,\n",
       " 0.997711756437711,\n",
       " 0.24597593722321714,\n",
       " 0.9208342378234267,\n",
       " 0.12134152227482771,\n",
       " 0.868291279094362,\n",
       " 0.9999916805472459,\n",
       " 0.2535255451473161,\n",
       " 0.9999952159103144,\n",
       " 0.9565077917045303,\n",
       " 0.9987366709241077,\n",
       " 0.9999798259667277,\n",
       " 0.9969786601304197,\n",
       " 0.9773346216961797,\n",
       " 0.9999279865327992,\n",
       " 0.9998279633081985,\n",
       " 0.9992133816901781,\n",
       " 0.9893492745947742,\n",
       " 0.974368874301971,\n",
       " 0.9946359344773203,\n",
       " 0.9766601594728083,\n",
       " 0.9698647858015144,\n",
       " 0.9071933883530168,\n",
       " 0.973969876701089,\n",
       " 0.9999654865323041,\n",
       " 0.0035871179677431637,\n",
       " 0.9998244736020409,\n",
       " 0.7910095337644091,\n",
       " 0.99391951105083,\n",
       " 0.8429275351289792,\n",
       " 0.9785180097683315,\n",
       " 0.020611333489189528,\n",
       " 0.9999968641968142,\n",
       " 0.9999969979260422,\n",
       " 0.995305215951144,\n",
       " 0.995469337520839,\n",
       " 0.9980589802233542,\n",
       " 0.9996292428023638,\n",
       " 0.9459919314951841,\n",
       " 0.5060376105509994,\n",
       " 0.9092068304356984,\n",
       " 0.9998647643704693,\n",
       " 0.02215328164761595,\n",
       " 0.9998339414824802,\n",
       " 0.9998499629873814,\n",
       " 0.9999355775184449,\n",
       " 0.5916815933665296,\n",
       " 0.9998313008853312,\n",
       " 0.9998212183114832,\n",
       " 0.9949594766455201,\n",
       " 0.987079823395409,\n",
       " 0.0006443619544634759,\n",
       " 0.9587784973822377,\n",
       " 0.8114590315579524,\n",
       " 0.9994355117391757,\n",
       " 0.7823565136653283,\n",
       " 0.9649817694347373,\n",
       " 0.9996034523693563,\n",
       " 0.8918276101796605,\n",
       " 0.9998836816600682,\n",
       " 0.9995812491761213,\n",
       " 0.9957913535035563,\n",
       " 0.9624561901804175,\n",
       " 0.9938707727366665,\n",
       " 0.9998876156278406,\n",
       " 0.8876742580131897,\n",
       " 0.9999943849411242,\n",
       " 0.9999508480942639,\n",
       " 0.9932979140336508,\n",
       " 0.9915539869847754,\n",
       " 0.9999912412099421,\n",
       " 0.9841994439507534,\n",
       " 0.9193715279819697,\n",
       " 0.6738706386167655,\n",
       " 0.11162029136586311,\n",
       " 0.9991458083396481,\n",
       " 0.986350111086094,\n",
       " 0.9997572403547528,\n",
       " 0.9161136629260462,\n",
       " 0.9998580219013188,\n",
       " 0.9981612481812024,\n",
       " 0.005518446246165641,\n",
       " 0.9999999848592109,\n",
       " 0.9997518957003616,\n",
       " 0.9999795308149216,\n",
       " 0.9940554325208804,\n",
       " 0.8769963267451515,\n",
       " 0.9963829477418896,\n",
       " 0.9300565640467933,\n",
       " 0.9991219097498458,\n",
       " 0.9989405352943993,\n",
       " 0.9987644904689391,\n",
       " 0.9493187156442864,\n",
       " 0.1608485815013963,\n",
       " 0.00046324332698198273,\n",
       " 0.9673468377019527,\n",
       " 0.8222469641069178,\n",
       " 0.99993110049004,\n",
       " 0.9998343257399255,\n",
       " 0.9941901137157819,\n",
       " 0.9005436558147774,\n",
       " 0.9999998871248811,\n",
       " 0.9999864554179099,\n",
       " 0.5211339391111629,\n",
       " 0.9998597194459728,\n",
       " 0.981684784283308,\n",
       " 0.9769926472931291,\n",
       " 0.36640574438163326,\n",
       " 0.45349925670546337,\n",
       " 0.9942208579710272,\n",
       " 0.9752936356856167,\n",
       " 0.998801202694664,\n",
       " 0.9973216579156416,\n",
       " 0.9534787541036567,\n",
       " 0.9972865087272892,\n",
       " 0.9999133242627851,\n",
       " 0.181504446223885,\n",
       " 1.6677739836665848e-05,\n",
       " 0.0731772072103812,\n",
       " 0.11669018349987051,\n",
       " 0.9999611152909229,\n",
       " 0.9992809932819533,\n",
       " 0.9998380419124352,\n",
       " 0.46391088661986046,\n",
       " 0.8071821638849218,\n",
       " 0.004807295020775422,\n",
       " 0.9938132765737921,\n",
       " 0.9999993071803664,\n",
       " 0.9999985237340507,\n",
       " 0.821929059381857,\n",
       " 0.9007077275691511,\n",
       " 0.41272467269095625,\n",
       " 0.11817245327752784,\n",
       " 0.9601334379215872,\n",
       " 0.9978608545457204,\n",
       " 0.9523850628493774,\n",
       " 0.9095595499363087,\n",
       " 0.6866114067670708,\n",
       " 0.9439589206577564,\n",
       " 0.9999883382805178,\n",
       " 0.9947396658717698,\n",
       " 0.9385889750561773,\n",
       " 0.8307572681683488,\n",
       " 0.9987873196633857,\n",
       " 0.9971027606433819,\n",
       " 0.8905397604120776,\n",
       " 0.5490899670435767,\n",
       " 0.9999865358969942,\n",
       " 0.9625274725859448,\n",
       " 0.9495574108276231,\n",
       " 0.9950152039203239,\n",
       " 0.9455981770719224,\n",
       " 0.9999998907370093,\n",
       " 0.9723492187767031,\n",
       " 0.9989355671371014,\n",
       " 0.8552392583152943,\n",
       " 0.9999807306679049,\n",
       " 0.1288497971938381,\n",
       " 0.980176903735901,\n",
       " 0.9999889992442134,\n",
       " 0.9999916066225263,\n",
       " 0.999776969600529,\n",
       " 0.9999847674670505,\n",
       " 0.7961223615416905,\n",
       " 0.9999818550341246,\n",
       " 0.7211343582911208,\n",
       " 0.9999352567407802,\n",
       " 0.9969358857475776,\n",
       " 0.9971203991165747,\n",
       " 0.9691308195314992,\n",
       " 0.9456472201168167,\n",
       " 0.9970562161373258,\n",
       " 0.8034517159495985,\n",
       " 0.9999934551339684,\n",
       " 0.9968423360289104,\n",
       " 0.3633162193608144,\n",
       " 0.11950515730790663,\n",
       " 0.8868442867386149,\n",
       " 0.9999838980619593,\n",
       " 0.9999926953791637,\n",
       " 0.012225361544051582,\n",
       " 0.45596902272781203,\n",
       " 0.9935400725913562,\n",
       " 0.8962331404755587,\n",
       " 0.9950510896401653,\n",
       " 0.999999862050171,\n",
       " 0.9979718838945478,\n",
       " 0.00026498470146021815,\n",
       " 0.9999973820779656,\n",
       " 0.9266083406898646,\n",
       " 0.9999993403717949,\n",
       " 0.0037210614311810413,\n",
       " 0.9999983575149912,\n",
       " 0.9024215772593136,\n",
       " 0.9999410446845597,\n",
       " 0.3325480799143172,\n",
       " 0.992060963397388,\n",
       " 0.998241036240519,\n",
       " 0.0001844753752537159,\n",
       " 0.9996580990908353,\n",
       " 0.9943880240827476,\n",
       " 0.023628721766007554,\n",
       " 0.9997842615397191,\n",
       " 0.9200091651136239,\n",
       " 0.6824048563816262,\n",
       " 0.5567451031869861,\n",
       " 0.9970723952445758,\n",
       " 0.5909767441729419,\n",
       " 0.9955742803870703,\n",
       " 0.9999772578991435,\n",
       " 0.08227525987965857,\n",
       " 0.8746274263264342,\n",
       " 0.999909387851865,\n",
       " 0.00012689570057095392,\n",
       " 0.9433014630499319,\n",
       " 0.988115998713174,\n",
       " 0.08197482583475883,\n",
       " 0.19587801948399244,\n",
       " 0.9996479106971891,\n",
       " 0.9553841312767937,\n",
       " 0.9982461107725001,\n",
       " 0.9999996493906566,\n",
       " 0.17442030270385445,\n",
       " 0.9999973435271529,\n",
       " 0.9999313867479432,\n",
       " 0.6618672452511762,\n",
       " 0.4129300322600245,\n",
       " 0.10046770881937767,\n",
       " 0.9999994343307947,\n",
       " 0.5595654714500321,\n",
       " 0.013580830925224854,\n",
       " 0.9924476338909071,\n",
       " 0.999998591408053,\n",
       " 0.9931744505470884,\n",
       " 0.9996243073366231,\n",
       " 0.9503819201765741,\n",
       " 0.9999366244469913,\n",
       " 0.031402392691605274,\n",
       " 0.9683485860595252,\n",
       " 0.9998955319276311,\n",
       " 0.8768094885319433,\n",
       " 0.9831651150026419,\n",
       " 0.9999753647412768,\n",
       " 0.9993144159231294,\n",
       " 0.9466281704434152,\n",
       " 0.9821239900016961,\n",
       " 0.5387226529552204,\n",
       " 0.04853925738119615,\n",
       " 0.5743983130203059,\n",
       " 0.032132347704764544,\n",
       " 0.996190408042652,\n",
       " 0.9983830929277226,\n",
       " 0.999851345628002,\n",
       " 0.05276222153314529,\n",
       " 0.7755752816075331,\n",
       " 0.9999995364217167,\n",
       " 0.7913542555820395,\n",
       " 0.9995262063988444,\n",
       " 0.26882273848535426,\n",
       " 0.9997238696758953,\n",
       " 0.9881221303707738,\n",
       " 0.012284911769881196,\n",
       " 0.592969734548492,\n",
       " 0.8863644106339118,\n",
       " 0.7483230674834183,\n",
       " 0.8456787322867899,\n",
       " 0.11344932538683215,\n",
       " 0.9775980284248386,\n",
       " 0.3459887859670165,\n",
       " 0.99690532713208,\n",
       " 0.9999999186075756,\n",
       " 0.9993050161603064,\n",
       " 0.9999922704515378,\n",
       " 0.07565425623407182,\n",
       " 0.3298729810516039,\n",
       " 0.0936188013531447,\n",
       " 0.9998015304020658,\n",
       " 0.9999991346390088,\n",
       " 0.6924105573084646,\n",
       " 0.9970008067821144,\n",
       " 0.9342938515808875,\n",
       " 0.9395637111369374,\n",
       " 0.9991640278407123,\n",
       " 0.5799878642322486,\n",
       " 0.5737187392359776,\n",
       " 0.9999217277737674,\n",
       " 0.9987654137205775,\n",
       " 0.9577646712975593,\n",
       " 0.999811141038154,\n",
       " 0.9997503246693108,\n",
       " 0.9998476757756388,\n",
       " 0.7335824253153955,\n",
       " 0.999972290191672,\n",
       " 0.016337023800860596,\n",
       " 0.12923262879116137,\n",
       " 0.2842322464952016,\n",
       " 0.7318771592080329,\n",
       " 0.36222452792840126,\n",
       " 0.9471358486671602,\n",
       " 0.998058078302403,\n",
       " 0.9999999027238174,\n",
       " 0.5835391512834198,\n",
       " 0.00017321183448529184,\n",
       " 0.999992450546688,\n",
       " 0.10324048728261304,\n",
       " 0.9997062422548302,\n",
       " 0.11058537378543123,\n",
       " 0.9743763284549166,\n",
       " 0.8325022145413725,\n",
       " 0.9981176774693526,\n",
       " 0.642820737962316,\n",
       " 0.9984690987154394,\n",
       " 0.9999917356427109,\n",
       " 0.17359596185017506,\n",
       " 0.9984765008444635,\n",
       " 0.9955173748192947,\n",
       " 0.9948544683913623,\n",
       " 0.0256371165044397,\n",
       " 0.5060933387622124,\n",
       " 0.7245032585039637,\n",
       " 0.988937635591938,\n",
       " 0.7481690152188587,\n",
       " 0.9997246244239323,\n",
       " 0.9758580353222985,\n",
       " 0.8913030847943241,\n",
       " 6.454640491312914e-05,\n",
       " 0.9999680674121221,\n",
       " 0.00530178770859106,\n",
       " 0.6523618227110668,\n",
       " 0.9999530576240054,\n",
       " 0.9970077580673542,\n",
       " 0.9999142777394758,\n",
       " 2.5170524047751094e-06,\n",
       " 0.9367641826383,\n",
       " 0.3269256453644968,\n",
       " 0.9998777514017517,\n",
       " 0.01766887679315456,\n",
       " 0.8675497775088982,\n",
       " 0.9841848144858134,\n",
       " 0.00018440690269077624,\n",
       " 0.8232976144591392,\n",
       " 0.581152868559407,\n",
       " 0.7851389551939844,\n",
       " 0.9963519820725231,\n",
       " 0.9676380100837828,\n",
       " 0.977091395512666,\n",
       " 1.3796785381097706e-06,\n",
       " 0.9704827887885487,\n",
       " 0.9560207421129595,\n",
       " 0.9558910175309684,\n",
       " 0.9888212307050872,\n",
       " 0.9941546224267506,\n",
       " 0.999057723512968,\n",
       " 0.8851254111032958,\n",
       " 0.10317076304712294,\n",
       " 0.5386310857452633,\n",
       " 0.9971391175069222,\n",
       " 0.6919173240522508,\n",
       " 0.9709850728012988,\n",
       " 0.9999927075922089,\n",
       " 0.0020804690894912947,\n",
       " 0.9651371002019082,\n",
       " 0.999889569878764,\n",
       " 0.9731055213975524,\n",
       " 0.9973035057342136,\n",
       " 0.2079736014553114,\n",
       " 0.9999319509102415,\n",
       " 0.9030352498534114,\n",
       " 0.9998969551554171,\n",
       " 0.7278298964039881,\n",
       " 0.9989682339603791,\n",
       " 0.9077848940860468,\n",
       " 0.9920444163336883,\n",
       " 0.24934653162753315,\n",
       " 0.11592305476247321,\n",
       " 0.9532818066017964,\n",
       " 0.03212598233105808,\n",
       " 0.9875369573121953,\n",
       " 0.9999951108098301,\n",
       " 0.13418070289797976,\n",
       " 2.8317632396886577e-05,\n",
       " 0.7390147605620647,\n",
       " 0.9921769072607526,\n",
       " 0.9990320002248934,\n",
       " 0.8564547107640357,\n",
       " 0.999998678522868,\n",
       " 0.995517017810517,\n",
       " 0.9402711350535059,\n",
       " 0.9999990796818169,\n",
       " 0.33107615815250424,\n",
       " 0.9924295457255421,\n",
       " 0.05540645804212253,\n",
       " 0.9800125505689715,\n",
       " 0.0002600877052642485,\n",
       " 0.9999950348183888,\n",
       " 0.9983476261378673,\n",
       " 0.9337859574736191,\n",
       " 0.9748551557389976,\n",
       " 0.8447163328297884,\n",
       " 0.9999999998577613,\n",
       " 0.7680509010944385,\n",
       " 0.9991618667458587,\n",
       " 0.9999225709319888,\n",
       " 0.17999411103476107,\n",
       " 0.07654354541780517,\n",
       " 0.353497043230296,\n",
       " 0.9942027422269686,\n",
       " 0.9997924752140057,\n",
       " 0.9999995265664222,\n",
       " 0.9994829765265297,\n",
       " 0.9999358465867878,\n",
       " 0.9991423363083994,\n",
       " 0.21988994648338217,\n",
       " 0.9999997422685711,\n",
       " 0.9940362177781834,\n",
       " 0.1059248556303829,\n",
       " 0.5833121502830205,\n",
       " 0.9952369378975116,\n",
       " 0.9902840589398106,\n",
       " 0.9999464237410938,\n",
       " 0.9541505427435407,\n",
       " 0.302278928071497,\n",
       " 0.25666821072334883,\n",
       " 0.9983519651692249,\n",
       " 0.9627402425124569,\n",
       " 0.9999999809337373,\n",
       " 0.06983070855908578,\n",
       " 0.999998513638123,\n",
       " 0.9812758620017573,\n",
       " 0.12816864087022764,\n",
       " 0.643657827255841,\n",
       " 0.8775681145719734,\n",
       " 0.929850050475773,\n",
       " 0.7691259760505382,\n",
       " 0.9271616564777754,\n",
       " 2.736863984421513e-09,\n",
       " 0.9999995874648905,\n",
       " 0.9994342475179226,\n",
       " 0.9574737652084728,\n",
       " 0.9943102405974186,\n",
       " 0.999960189754827,\n",
       " 0.9851620391062911,\n",
       " 0.9999977210012283,\n",
       " 0.9999621896572358,\n",
       " 0.9999968676552614,\n",
       " 0.7275639431036974,\n",
       " 0.9998343945114668,\n",
       " 0.9999478615283651,\n",
       " 0.04790700727577002,\n",
       " 3.5248716204751605e-05,\n",
       " 0.9997427107411371,\n",
       " 0.9973507605168057,\n",
       " 0.964812286481031,\n",
       " 0.9947412568440648,\n",
       " 0.7416397331453078,\n",
       " 0.5200057932697068,\n",
       " 2.7069047723304724e-09,\n",
       " 0.9933195178199969,\n",
       " 0.9922330033445196,\n",
       " 0.9999994038178442,\n",
       " 0.9841879555852157,\n",
       " 0.01619235733952161,\n",
       " 0.9988389804202956,\n",
       " 0.9998737515500377,\n",
       " 0.9956607698557287,\n",
       " 0.9995205360638043,\n",
       " 0.9998968250264527,\n",
       " 0.9217650800335495,\n",
       " 0.9998959771125472,\n",
       " 0.024350172555543444,\n",
       " 0.968858156672635,\n",
       " 0.9997531642483706,\n",
       " 0.9999807554071314,\n",
       " 0.873710673498781,\n",
       " 0.5427838363466047,\n",
       " 1.7301013408947535e-05,\n",
       " 0.9764734657206148,\n",
       " 0.9911858951097287,\n",
       " 0.9999921006000658,\n",
       " 0.8810134185475715,\n",
       " 0.0147599340318059,\n",
       " 0.34012866253235535,\n",
       " 0.9999669726393849,\n",
       " 0.9996532090771714,\n",
       " 0.7545464783783102,\n",
       " 0.9999145571790304,\n",
       " 0.4731993795577144,\n",
       " 0.9992741499021541,\n",
       " 0.2862465658617598,\n",
       " 0.9998574359401853,\n",
       " 0.9866334970785509,\n",
       " 0.9999918483050696,\n",
       " 0.015259338112684087,\n",
       " 0.9898483624963159,\n",
       " 0.9867846388823459,\n",
       " 0.9999560882518052,\n",
       " 0.9986939735876345,\n",
       " 0.9999693045494309,\n",
       " 0.9981981218893051,\n",
       " 0.9899370244466733,\n",
       " 0.9919466034671206,\n",
       " 4.115407424450669e-05,\n",
       " 0.0003819614684816666,\n",
       " 0.0004213081019072225,\n",
       " 0.9999817824650817,\n",
       " 0.9986927625355742,\n",
       " 0.9998128548472609,\n",
       " 0.08982255458695385,\n",
       " 0.7178455613362921,\n",
       " 0.9999159078921521,\n",
       " 0.6771253457514755,\n",
       " 0.959122811774318,\n",
       " 0.9157264620198304,\n",
       " 3.6070275937472265e-09,\n",
       " 0.9999303137233986,\n",
       " 0.9999999823635458,\n",
       " 0.9999948772252827,\n",
       " 0.9999867406275634,\n",
       " 0.7700101420012906,\n",
       " 0.004497464775201507,\n",
       " 0.05589574079962403,\n",
       " 0.7375810144683038,\n",
       " 0.999999125360951,\n",
       " 0.9993541280895945,\n",
       " 0.9999635170522315,\n",
       " 0.9999912185274569,\n",
       " 0.003012709923570086,\n",
       " 3.826275233059445e-06,\n",
       " 0.029395034204668047,\n",
       " 0.7895251928295564,\n",
       " 0.06581945891616009,\n",
       " 0.999647432816223,\n",
       " 0.001963267635677619,\n",
       " 0.9993245893772844,\n",
       " 0.9999994548022347,\n",
       " 0.19756814703682213,\n",
       " 0.9061748569802001,\n",
       " 0.9986911607322815,\n",
       " 0.21484938118051589,\n",
       " 0.9942650517758991,\n",
       " 0.9531434364936532,\n",
       " 0.999961670465113,\n",
       " 0.32811244640554066,\n",
       " 0.8901184509394654,\n",
       " 0.9371648153573707,\n",
       " 0.6742026379858472,\n",
       " 0.9999839233154355,\n",
       " 0.9999999266965991,\n",
       " 0.9969920564686132,\n",
       " 6.91203082888704e-05,\n",
       " 0.7114497816848441,\n",
       " 0.9983116941863998,\n",
       " 0.9999435141342942,\n",
       " 0.9999999006005994,\n",
       " 0.9474104823489721,\n",
       " 0.9948953143238258,\n",
       " 0.4547731478764112,\n",
       " 0.9999573885340896,\n",
       " 0.9999994778860258,\n",
       " 0.9998855857102538,\n",
       " 0.8476748073623629,\n",
       " 0.9968776742324797,\n",
       " 0.9999082483211769,\n",
       " 0.9969255972106139,\n",
       " 0.9138316294613934,\n",
       " 0.743814590174494,\n",
       " 0.9745863077622977,\n",
       " 0.9509493439934706,\n",
       " 0.9965748438445263,\n",
       " 0.9798265124365747,\n",
       " 0.9830521777390975,\n",
       " 0.040850889770856706,\n",
       " 0.9999310429958339,\n",
       " 0.9999940274806205,\n",
       " 0.9721935509704823,\n",
       " 0.33985885023954004,\n",
       " 0.1523968350381452,\n",
       " 0.9985546123853924,\n",
       " 0.9996773363656576,\n",
       " 0.999973486960338,\n",
       " 0.99600979630637,\n",
       " 0.9992120819343339,\n",
       " 0.998450109532619,\n",
       " 0.8036538184715186,\n",
       " 0.9999979782312575,\n",
       " 0.9999953425309648,\n",
       " 0.9983499525263886,\n",
       " 0.9975658495766898,\n",
       " 0.9999755337129401,\n",
       " 0.16301238415102498,\n",
       " 0.08469462672424535,\n",
       " 0.9887574562627887,\n",
       " 0.9999920708956488,\n",
       " 0.004136851382408208,\n",
       " 0.9999992201691379,\n",
       " 0.9969487303492206,\n",
       " 0.9999696564676586,\n",
       " 0.9990699834391215,\n",
       " 0.07703336486827124,\n",
       " 0.9992657710034097,\n",
       " 0.5488878178875022,\n",
       " 0.9999988763046151,\n",
       " 0.00037990395394030366,\n",
       " 0.995757276138084,\n",
       " 0.9998731893424849,\n",
       " 0.9776188518257053,\n",
       " 0.9999930772722329,\n",
       " 0.9824399097369951,\n",
       " 0.9997274209846501,\n",
       " 0.1285265708629284,\n",
       " 0.2887471636061223,\n",
       " 0.6832660178214899,\n",
       " 0.801462146219359,\n",
       " 0.9995643286340576,\n",
       " 0.9999992427085458,\n",
       " 0.9999422242911723,\n",
       " 0.9999305275003262,\n",
       " 0.449690437679044,\n",
       " 0.9952798498559478,\n",
       " 0.9956202928336353,\n",
       " 0.9999856456145592,\n",
       " 0.0005058550079851728,\n",
       " 0.9983624867980935,\n",
       " 0.9333297720256037,\n",
       " 0.06770618637279653,\n",
       " 0.9999287190875454,\n",
       " 0.9660594368048115,\n",
       " 0.9998984843504214,\n",
       " 0.508460468209968,\n",
       " 0.00040407890723163746,\n",
       " 0.8118487433340283,\n",
       " 0.34586630346744773,\n",
       " 0.9440351981970755,\n",
       " 0.5861205027479294,\n",
       " 0.9895276760047348,\n",
       " 0.9999467790514731,\n",
       " 0.8528641871982882,\n",
       " 0.9480277076196922,\n",
       " 0.9996166840161554,\n",
       " 0.9998514903102858,\n",
       " 0.9999746084738349,\n",
       " 0.9756714825520709,\n",
       " 0.9997837313327208,\n",
       " 0.8029295758034649,\n",
       " 0.9977367131578272,\n",
       " 0.9967929440451025,\n",
       " 0.9998751629315155,\n",
       " 0.9981497432263469,\n",
       " 0.6647466914972127,\n",
       " 6.882047315378575e-07,\n",
       " 0.862924031260278,\n",
       " 0.9996967513184575,\n",
       " 0.9999989604085046,\n",
       " 0.9929097238114204,\n",
       " 0.17914163651917406,\n",
       " 0.9997441590006984,\n",
       " 0.9982065905296852,\n",
       " 0.9957653944835466,\n",
       " 0.15909602041223694,\n",
       " 0.659569512915948,\n",
       " 1.0609291313429073e-09,\n",
       " 0.9996418478767599,\n",
       " 0.2086937745945952,\n",
       " 0.999158667519234,\n",
       " 1.6993830219281108e-05,\n",
       " 0.40904833204065605,\n",
       " 0.9998517043077478,\n",
       " 0.9994954413728111,\n",
       " 2.5081406211580117e-08,\n",
       " 0.16094762085712722,\n",
       " 0.9971546497793436,\n",
       " 0.21892512620981935,\n",
       " 0.7133031845773908,\n",
       " 0.9960988663215663,\n",
       " 0.9696391894132237,\n",
       " 0.9002800377725895,\n",
       " 0.9998150582787247,\n",
       " 0.005702508099905328,\n",
       " 0.8212635630256608,\n",
       " 0.0035664098638498102,\n",
       " 0.9999832877613789,\n",
       " 0.8993980276361512,\n",
       " 0.9999970545981608,\n",
       " 0.18319159138499258,\n",
       " 0.7559636129655612,\n",
       " 0.985438866931668,\n",
       " 0.9954474181776867,\n",
       " 0.057208959886945236,\n",
       " 0.9999977046592213,\n",
       " 0.006271855797779135,\n",
       " 0.05727180301908996,\n",
       " 0.9976228494783562,\n",
       " 0.9977185358211983,\n",
       " 0.7132689539707586,\n",
       " 0.014365086741845294,\n",
       " 0.9988673018946896,\n",
       " 0.9765510383317783,\n",
       " 0.9999556267389645,\n",
       " 0.9960483766792859,\n",
       " 0.7177374637923503,\n",
       " 0.2120897176625488,\n",
       " 0.9999685639669504,\n",
       " 0.9513669894864111,\n",
       " 0.9853262160383744,\n",
       " 0.9999966319825015,\n",
       " 0.7118808600658635,\n",
       " 0.9958360756953779,\n",
       " 0.9974445674450804,\n",
       " 0.8606593265990379,\n",
       " 0.18465999440022804,\n",
       " 0.9855879541446366,\n",
       " 0.7732852094692112,\n",
       " 0.05687427621869432,\n",
       " 0.999997963619204,\n",
       " 0.07434666541210169,\n",
       " 0.9999440371239113,\n",
       " 0.6934856051144207,\n",
       " 0.10608103398412605,\n",
       " 2.0738621927260728e-08,\n",
       " 0.9999051159732001,\n",
       " 0.9955350637663568,\n",
       " 0.9393064192636317,\n",
       " 0.936292459975654,\n",
       " 0.969660152126733,\n",
       " 0.9998331231707525,\n",
       " 0.9634701961574633,\n",
       " 0.7407209307921622,\n",
       " 0.9701312361890448,\n",
       " 0.5176149100879803,\n",
       " 0.9869815462439712,\n",
       " 0.9994317060261312,\n",
       " 0.9543007211060702,\n",
       " 0.9947511094251745,\n",
       " 0.840497614081503,\n",
       " 0.9858175380816888,\n",
       " 0.9999860939437055,\n",
       " 0.9895648566808298,\n",
       " 0.9999974774983033,\n",
       " 0.9989591171992095,\n",
       " 0.9796257765403986,\n",
       " 0.9925780519509142,\n",
       " 0.023251238624750363,\n",
       " 0.6534024283763847,\n",
       " 0.9257348245396941,\n",
       " 0.9963922154812987,\n",
       " 0.9956911984321044,\n",
       " 0.9908710242750376,\n",
       " 0.9351478430051887,\n",
       " 0.5598167681861526,\n",
       " 0.1633813234189058,\n",
       " 0.9999985385432058,\n",
       " 0.8631683088966678,\n",
       " 0.9999999056636366,\n",
       " 0.9616296702889281,\n",
       " 0.9999971090457588,\n",
       " 0.999849965214032,\n",
       " 0.9462199929959022,\n",
       " 0.9988957960577999,\n",
       " 0.017813009447535166,\n",
       " 0.9999999520219495,\n",
       " 0.024350639197080623,\n",
       " 0.5172215412967242,\n",
       " 0.9998427147292377,\n",
       " 0.9993818076843197,\n",
       " 0.3692324158851812,\n",
       " 0.9999109199658631,\n",
       " 0.9892523448283274,\n",
       " 0.9999286476347474,\n",
       " 0.9999427438684932,\n",
       " 0.9993295654069101,\n",
       " 0.9999918015881606,\n",
       " 0.9774448726785369,\n",
       " 0.9991902246004752,\n",
       " 0.9108203910949221,\n",
       " 0.9999392966785907,\n",
       " 0.9993352030660757,\n",
       " 0.9999293052720502,\n",
       " 0.40385947863211197,\n",
       " 0.4182034967753919,\n",
       " 0.999629933150069,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob_all = model.predict_proba(X)\n",
    "list(y_prob_all[y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06282380224009473"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_prob_all[y]<0.01) / len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 2\n",
    "### Network 1\n",
    "-  Create neural network with single hidden layer\n",
    "-  Use **Accuracy** as metric of performance  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['    def base_model(self, nodes, num_output=31, kernel_regularizer=None, layer_id=None):\\n',\n",
       "  '        model = Sequential()\\n',\n",
       "  '        for prev_node, node in zip(nodes[:-1], nodes[1:]):\\n',\n",
       "  '            layer_name = None\\n',\n",
       "  '            if layer_id is not None:\\n',\n",
       "  '                layer_name = \"{}-in-{}-n-{}\".format(layer_id, str(prev_node), str(prev_node))\\n',\n",
       "  \"            model.add(Dense(node, activation='relu', kernel_regularizer=kernel_regularizer,\\n\",\n",
       "  '                            input_dim=prev_node, name=layer_name))\\n',\n",
       "  \"        model.add(Dense(num_output, activation='sigmoid', name='features'))\\n\",\n",
       "  \"        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\\n\",\n",
       "  '        return model\\n'],\n",
       " 38)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(nn.train)\n",
    "inspect.getsourcelines(nn.base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811/811 [==============================] - 0s 589us/step\n",
      "Test loss: 0.07687149639672033\n",
      "Test accuracy: 0.9741458293101055\n"
     ]
    }
   ],
   "source": [
    "nn.train(X, y, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    ">  With just single layer we are getting accuracy of 97% Epoch=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Step 3\n",
    "### Network 2\n",
    "-  Use the same model as step 1 for neurons  = 5 to 32\n",
    "-  Measure accuracy at each step to find optimal number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['    def train_network_2(self, X, y):\\n',\n",
       "  '        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=5)\\n',\n",
       "  '        for P in range(2, 12):\\n',\n",
       "  '            for num_layers in range(1, 2):\\n',\n",
       "  '                nodes = [64] + [int(P / 2)] * num_layers\\n',\n",
       "  '                print(nodes)\\n',\n",
       "  '                model = self.base_model(nodes)\\n',\n",
       "  '                print(model.summary())\\n',\n",
       "  '                summary = model.fit(X_train, y_train, epochs=10, verbose=0)\\n',\n",
       "  '                score = model.evaluate(X_test, y_test)\\n',\n",
       "  \"                print('Test loss:', score[0])\\n\",\n",
       "  \"                print('Test accuracy:', score[1])\\n\"],\n",
       " 112)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(nn.train_network_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 1]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                62        \n",
      "=================================================================\n",
      "Total params: 127\n",
      "Trainable params: 127\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 0s 52us/step\n",
      "Test loss: 0.13744815848087757\n",
      "Test accuracy: 0.9677419066429138\n",
      "[64, 1]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                62        \n",
      "=================================================================\n",
      "Total params: 127\n",
      "Trainable params: 127\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 0s 52us/step\n",
      "Test loss: 0.1211563661790094\n",
      "Test accuracy: 0.9695561404703401\n",
      "[64, 2]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                93        \n",
      "=================================================================\n",
      "Total params: 223\n",
      "Trainable params: 223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 175us/step\n",
      "Test loss: 0.10961133357069441\n",
      "Test accuracy: 0.9694208700373423\n",
      "[64, 2]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                93        \n",
      "=================================================================\n",
      "Total params: 223\n",
      "Trainable params: 223\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 141us/step\n",
      "Test loss: 0.108455138888392\n",
      "Test accuracy: 0.9705985256998658\n",
      "[64, 3]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                124       \n",
      "=================================================================\n",
      "Total params: 319\n",
      "Trainable params: 319\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 208us/step\n",
      "Test loss: 0.08931560870985945\n",
      "Test accuracy: 0.9714181157656944\n",
      "[64, 3]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 3)                 195       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                124       \n",
      "=================================================================\n",
      "Total params: 319\n",
      "Trainable params: 319\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 0s 75us/step\n",
      "Test loss: 0.08955401260833994\n",
      "Test accuracy: 0.9712510157466582\n",
      "[64, 4]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                155       \n",
      "=================================================================\n",
      "Total params: 415\n",
      "Trainable params: 415\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 277us/step\n",
      "Test loss: 0.08215475803931241\n",
      "Test accuracy: 0.9734471866398771\n",
      "[64, 4]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                155       \n",
      "=================================================================\n",
      "Total params: 415\n",
      "Trainable params: 415\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 231us/step\n",
      "Test loss: 0.0885079911021864\n",
      "Test accuracy: 0.9712987643894916\n",
      "[64, 5]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                186       \n",
      "=================================================================\n",
      "Total params: 511\n",
      "Trainable params: 511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 0s 83us/step\n",
      "Test loss: 0.08039291372102766\n",
      "Test accuracy: 0.9737416020331268\n",
      "[64, 5]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "None-in-64-n-64 (Dense)      (None, 5)                 325       \n",
      "_________________________________________________________________\n",
      "features (Dense)             (None, 31)                186       \n",
      "=================================================================\n",
      "Total params: 511\n",
      "Trainable params: 511\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "4054/4054 [==============================] - 1s 256us/step\n",
      "Test loss: 0.07511871197083106\n",
      "Test accuracy: 0.9746248383620836\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def base_model(nodes, num_output=31, kernel_regularizer=None, layer_id=None):\n",
    "    model = Sequential()\n",
    "    layer_name = None\n",
    "    for prev_node, node in zip(nodes[:-1], nodes[1:]):\n",
    "        layer_name = \"{}-in-{}-n-{}\".format(layer_id, str(prev_node), str(prev_node))\n",
    "        model.add(Dense(node, activation='relu', kernel_regularizer=kernel_regularizer,\n",
    "        input_dim=prev_node, name=layer_name))\n",
    "    model.add(Dense(num_output, activation='sigmoid', name='features'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_network_debug(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=5)\n",
    "    for P in range(2, 12):\n",
    "        for num_layers in range(1, 2):\n",
    "            nodes = [64] + [int(P / 2)] * num_layers\n",
    "            print(nodes)\n",
    "            model = base_model(nodes)\n",
    "            print(model.summary())\n",
    "            summary = model.fit(X_train, y_train, epochs=10, verbose=0)\n",
    "            score = model.evaluate(X_test, y_test)\n",
    "            print('Test loss:', score[0])\n",
    "            print('Test accuracy:', score[1])\n",
    "\n",
    "train_network_debug(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 1]\n",
      "4054/4054 [==============================] - 0s 113us/step\n",
      "Test loss: 0.12101917903177481\n",
      "Test accuracy: 0.9696277563773062\n",
      "[64, 1]\n",
      "4054/4054 [==============================] - 1s 343us/step\n",
      "Test loss: 0.12091939075935869\n",
      "Test accuracy: 0.9695004410913088\n",
      "[64, 2]\n",
      "4054/4054 [==============================] - ETA:  - 1s 318us/step\n",
      "Test loss: 0.10972485231850379\n",
      "Test accuracy: 0.9692060279622711\n",
      "[64, 2]\n",
      "4054/4054 [==============================] - 1s 165us/step\n",
      "Test loss: 0.10918649259566908\n",
      "Test accuracy: 0.9704791691483214\n",
      "[64, 3]\n",
      "4054/4054 [==============================] - 1s 298us/step\n",
      "Test loss: 0.08958604379010777\n",
      "Test accuracy: 0.9722615741248429\n",
      "[64, 3]\n",
      "4054/4054 [==============================] - 2s 382us/step\n",
      "Test loss: 0.08949642153534915\n",
      "Test accuracy: 0.9719751129376224\n",
      "[64, 4]\n",
      "4054/4054 [==============================] - 1s 367us/step\n",
      "Test loss: 0.09400902894340717\n",
      "Test accuracy: 0.9707338043369562\n",
      "[64, 4]\n",
      "4054/4054 [==============================] - ETA:  - 2s 421us/step\n",
      "Test loss: 0.07848305801379016\n",
      "Test accuracy: 0.9735665380160798\n",
      "[64, 5]\n",
      "4054/4054 [==============================] - 1s 149us/step\n",
      "Test loss: 0.07754468835695483\n",
      "Test accuracy: 0.97425084709239\n",
      "[64, 5]\n",
      "4054/4054 [==============================] - 1s 347us/step\n",
      "Test loss: 0.07540886808548823\n",
      "Test accuracy: 0.9746009679809837\n"
     ]
    }
   ],
   "source": [
    "nn.train_network_2(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    ">  ### Accuraccy is already at 96% in step 1, so we add regularize function to decrease accuracy and determine optimal number of neurons as 6\n",
    ">  ### P=6 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Step 4\n",
    "### Network 3\n",
    "-  Find optimal number of hidden layers with number of neurons as P/2\n",
    "-  Again, since the accuracy is very high, we will have to take regularizer function\n",
    "-  P=6, Number of neurons = P/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['    def train_network_3(self, X, y):\\n',\n",
       "  '        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=5)\\n',\n",
       "  '        P = 6\\n',\n",
       "  '        for num_layers in range(1, 10):\\n',\n",
       "  '            nodes = [64] + [int(P / 2)] * num_layers\\n',\n",
       "  '            model = self.base_model(nodes)\\n',\n",
       "  '            print(model.summary())\\n',\n",
       "  '            summary = model.fit(X_train, y_train, epochs=10, verbose=0)\\n',\n",
       "  '            score = model.evaluate(X_test, y_test)\\n',\n",
       "  \"            print('Test loss:', score[0])\\n\",\n",
       "  \"            print('Test accuracy:', score[1])\\n\"],\n",
       " 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(nn.train_network_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4054/4054 [==============================] - 0s 58us/step\n",
      "Test loss: 0.09179627784717595\n",
      "Test accuracy: 0.97132263212411\n",
      "4054/4054 [==============================] - 0s 62us/step\n",
      "Test loss: 0.08782961113689919\n",
      "Test accuracy: 0.9706621874890189\n",
      "4054/4054 [==============================] - 0s 69us/step\n",
      "Test loss: 0.11377906092117734\n",
      "Test accuracy: 0.9698903405672232\n",
      "4054/4054 [==============================] - 0s 77us/step\n",
      "Test loss: 0.12015273502499939\n",
      "Test accuracy: 0.9702404533693457\n",
      "4054/4054 [==============================] - 0s 82us/step\n",
      "Test loss: 0.13819008715704384\n",
      "Test accuracy: 0.9677419066429138\n",
      "4054/4054 [==============================] - 0s 93us/step\n",
      "Test loss: 0.1198273547908974\n",
      "Test accuracy: 0.9702165809298714\n",
      "4054/4054 [==============================] - 0s 104us/step\n",
      "Test loss: 0.121142026733517\n",
      "Test accuracy: 0.9700574382338315\n",
      "4054/4054 [==============================] - 0s 122us/step\n",
      "Test loss: 0.1381859623068474\n",
      "Test accuracy: 0.9677419066429138\n",
      "4054/4054 [==============================] - 1s 133us/step\n",
      "Test loss: 0.11945707400590842\n",
      "Test accuracy: 0.9698028180125199\n"
     ]
    }
   ],
   "source": [
    "nn.train_network_3(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    ">  ### Because of high accuracy, we can select number of layers=6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "## Step 5\n",
    "### Network 4\n",
    "-  Without any conditions, determine optimal architecture\n",
    "-  We can run the training module with P=6 and Layers=6\n",
    "-  Save model for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['    def train_with_callback(self, X, y, model_file: str = None, generate_plots=True):\\n',\n",
       "  '        if model_file is None:\\n',\n",
       "  '            model_file = self._model\\n',\n",
       "  '        plot_losses = TrainingPlot()\\n',\n",
       "  '        time_summary = TimeSummary()\\n',\n",
       "  \"        layer_names = ['features']\\n\",\n",
       "  '        num_nodes = 6\\n',\n",
       "  '        for layer_id in range(6, 7):\\n',\n",
       "  '            nodes = [64, num_nodes]\\n',\n",
       "  '            for prev_node, node in zip(nodes[:-1], nodes[1:]):\\n',\n",
       "  '                layer_name = None\\n',\n",
       "  '                if layer_id is not None:\\n',\n",
       "  '                    layer_name = \"{}-in-{}-n-{}\".format(layer_id, str(prev_node), str(prev_node))\\n',\n",
       "  '                    layer_names.append(layer_name)\\n',\n",
       "  '\\n',\n",
       "  '        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.01, random_state=5)\\n',\n",
       "  \"        with open(os.path.join(self._log_dir, 'metadata.tsv'), 'w') as f:\\n\",\n",
       "  '            np.savetxt(f, y_test)\\n',\n",
       "  '        tensorboard = TensorBoard(log_dir=self._log_dir, histogram_freq=2, batch_size=32,\\n',\n",
       "  '                                  write_graph=True,\\n',\n",
       "  '                                  write_grads=False, write_images=True, embeddings_freq=1,\\n',\n",
       "  \"                                  embeddings_layer_names=['features'],\\n\",\n",
       "  '                                  embeddings_data=X_test,\\n',\n",
       "  \"                                  update_freq='epoch')\\n\",\n",
       "  '        callbacks = [tensorboard, time_summary]\\n',\n",
       "  '        if generate_plots:\\n',\n",
       "  '            callbacks.append(plot_losses)\\n',\n",
       "  '        for layer in range(6, 7):\\n',\n",
       "  '            nodes = [64, num_nodes]\\n',\n",
       "  '            model = self.base_model(nodes, layer_id=\"l-{}\".format(layer))\\n',\n",
       "  '            summary = model.fit(X_train, y_train, epochs=100, verbose=0, validation_data=(X_test, y_test),\\n',\n",
       "  '                                callbacks=callbacks)\\n',\n",
       "  '            score = model.evaluate(X_test, y_test)\\n',\n",
       "  '            if generate_plots:\\n',\n",
       "  '                plot_training_summary(summary, time_summary)\\n',\n",
       "  '            score = model.evaluate(X_test, y_test)\\n',\n",
       "  '            model.save(model_file)\\n',\n",
       "  \"            print('Test loss:', score[0])\\n\",\n",
       "  \"            print('Test accuracy:', score[1])\\n\"],\n",
       " 61)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(nn.train_with_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-786875b5b4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_with_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/tf/models/model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tf/notebooks/NNKeras.py\u001b[0m in \u001b[0;36mtrain_with_callback\u001b[0;34m(self, X, y, model_file, generate_plots)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"l-{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             summary = model.fit(X_train, y_train, epochs=100, verbose=0, validation_data=(X_test, y_test),\n\u001b[0;32m---> 92\u001b[0;31m                                 callbacks=callbacks)\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgenerate_plots\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    979\u001b[0m                                     os.path.join(self.log_dir,\n\u001b[1;32m    980\u001b[0m                                                  'keras_embedding.ckpt'),\n\u001b[0;32m--> 981\u001b[0;31m                                     epoch)\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m           self.export_meta_graph(\n\u001b[0;32m-> 1196\u001b[0;31m               meta_graph_filename, strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         \u001b[0mclear_devices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m         \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m         strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1559\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1560\u001b[0m       \u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1561\u001b[0;31m       **kwargs)\n\u001b[0m\u001b[1;32m   1562\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[0;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, saver_def, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m         as_text=as_text)\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mscoped_meta_graph_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[0;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[1;32m     71\u001b[0m                                         text_format.MessageToString(graph_def))\n\u001b[1;32m     72\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nn.train_with_callback(X, y, '/tf/models/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "### In this dataset, our neural network reaches accuracy of 96% with sample data and 98% with full dataset. Hence it is necessary to validate the accuracy of data with trained model.\n",
    "### Keras allows callbacks for training visualization. Function below runs for 200 epochs with 32 neurons and saves the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "### Predict from souce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inspect.getsourcelines(nn.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nn.predict(\"/tf/dataset/ae002161.csv\", unique_classes, 5, \"/tf/models/model_full.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nn.predict(\"/tf/dataset/ae003852.csv\", unique_classes, 5, \"/tf/models/model_full.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    ">  ### As we can see that the accuracy is as expected for the first case but for second, it is less than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": [
    "## Step 6\n",
    "### Network 5\n",
    "-  We will use same training function with each column of y as 1D matrix\n",
    "-  Take average of each inidividual prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "inspect.getsourcelines(nn.single_output_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "avg_score = nn.single_output_score(X, y)\n",
    "avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "metadata": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    ">  ### Average accuracy of individual prediction is less than overall prediction accuracy. Hence, we can conclude that it is better to create one network with N output neurons than N networks each with one output neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "-  ### Generic module for generating optimized network\n",
    "-  ### Visualize t-SNE for hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation with linear classification methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "##############  Validation with linear classification methods\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "tmp = DecisionTreeClassifier(min_samples_leaf=10)\n",
    "tmp.fit(X_train, y_train)\n",
    "y_pred = tmp.predict(X_test)\n",
    "print('test', accuracy_score(y_pred, y_test))\n",
    "y_pred_train = tmp.predict(X_train)\n",
    "print('train', accuracy_score(y_pred_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "##############  Validation with linear classification methods\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train)\n",
    "# print(pca.components_)\n",
    "X_proj = pca.transform(X_test)\n",
    "\n",
    "f, ax = plt.subplots(2, sharex=True)\n",
    "f.set_figheight(10)\n",
    "ax[0].scatter(X_proj[:, 0], X_proj[:, 1], c=np.argmax(y_test, axis=1), alpha=0.9)\n",
    "ax[1].scatter(X_proj[:, 0], X_proj[:, 1], c=np.argmax(y_pred, axis=1), alpha=0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
